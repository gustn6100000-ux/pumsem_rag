# Step 2.4 엔티티 정규화 & 중복 제거 — 상세 구현 계획서

> **작성일**: 2026-02-11  
> **버전**: v1.2 (Codex 재검토 반영)  
> **입력**: `merged_entities.json` (25,708 엔티티, 28,734 관계)  
> **출력**: `normalized_entities.json` (~15,171 엔티티, ~28,000 관계)  
> **구현 파일**: `phase2_extraction/step4_normalizer.py` (신규 생성)

### 변경 이력

| 버전  | 날짜       | 변경 내용                                                                                                                      |
| :---: | ---------- | ------------------------------------------------------------------------------------------------------------------------------ |
| v1.0  | 2026-02-11 | 초안 작성                                                                                                                      |
| v1.1  | 2026-02-11 | Codex 실행타당성검토 6건 반영 — dedup 키 spec 포함, name_map 타입 안전화, 관계 dedup 키 확장, 수치 단일화, Section 예상치 보정 |
| v1.2  | 2026-02-11 | Codex 재검토 반영 — 근거 수치 정합화, Phase C 키 타입 안전화, 관계 dedup에 per_unit 추가, 출력 스키마(source_chunk_ids) 일치화 |

---

## 0. 데이터 현황 분석 (구현 전 팩트)

### 0.1 타입별 중복률

| 유형          |    전체    | 고유(이름) | 고유(이름+spec) | 중복률 | 정규화 후 예상 |
| ------------- | :--------: | :--------: | :-------------: | :----: | :------------: |
| **Labor**     |   4,270    |    337     |       337       | 92.1%  |      337       |
| **Equipment** |   2,706    |   1,197    |    **1,820**    | 55.8%  |     1,820      |
| **Section**   |   2,105    |    997     |       997       | 52.6%  |      997       |
| **Material**  |   2,618    |   1,293    |    **1,684**    | 50.6%  |     1,684      |
| **Standard**  |   1,102    |    571     |       569       | 48.2%  |      569       |
| **WorkType**  |   5,995    |   3,791    |    **4,552**    | 36.8%  |     4,552      |
| **Note**      |   6,912    |   5,176    |      5,212      | 25.1%  |     5,212      |
| **합계**      | **25,708** |     —      |        —        |   —    |  **~15,171**   |

> ⚠️ **Codex 지적 #2 반영**: WorkType/Equipment/Material은 `이름+spec` 기준으로 고유성을 판별해야 과잉 병합을 방지할 수 있음. 이름만 기준 시 1,782건 과잉 병합 위험.  
> ⚠️ **Codex 지적 #5 반영**: Section 고유 수를 1,043 → 997(실측 code 기준)로 보정.

> **핵심 인사이트**: Labor가 92.1%로 가장 높은 중복률. "보통인부"가 736번 등장 — 모든 청크에서 별도 엔티티로 추출되었기 때문. 정규화로 337개로 압축 가능.

### 0.2 공백 변형 현황

총 **435개 그룹**에서 같은 엔티티가 다른 공백 형태로 존재:

```
"초급 기능사 (측량)" / "초급 기능사(측량)" / "초급기능사 (측량)" / "초급기능사(측량)"
"덤 프 트 럭" / "덤프 트럭" / "덤프트럭"
"모 터 그 레이 더 ( 일 반 용 )" / "모터그레이더 (일반용)" / "모터그레이더(일반용)"
```

### 0.3 관계 방향 오류

총 **725건** 방향 오류:

| 패턴                                     | 건수  | 비율  | 처리 전략                          |
| ---------------------------------------- | :---: | :---: | ---------------------------------- |
| Equipment→Material (USES_MATERIAL)       |  291  | 40.1% | WorkType으로 source 교체           |
| Equipment→Labor (REQUIRES_LABOR)         |  156  | 21.5% | WorkType으로 source 교체           |
| Equipment→Equipment (REQUIRES_EQUIPMENT) |  93   | 12.8% | 첫 번째를 WorkType으로 재분류 시도 |
| WorkType→WorkType (HAS_NOTE)             |  64   | 8.8%  | target을 Note로 재분류             |
| Material→Equipment (REQUIRES_EQUIPMENT)  |  37   | 5.1%  | WorkType으로 source 교체           |
| Note→Note (HAS_NOTE)                     |  33   | 4.6%  | 관계 삭제                          |
| 기타                                     |  51   | 7.0%  | 개별 판단 후 삭제 또는 보정        |

### 0.4 수량 이상치

| 관계 유형          | 건수  | 최소  |     최대      |  P95  | 이상치(>P95×3) |
| ------------------ | :---: | :---: | :-----------: | :---: | :------------: |
| REQUIRES_LABOR     | 9,385 | 0.00  |    646.45     | 25.00 |     101건      |
| REQUIRES_EQUIPMENT | 1,472 | 0.00  | **1,042,936** | 1,100 |      39건      |
| USES_MATERIAL      | 1,640 | 0.00  |    28,728     | 75.00 |      37건      |

> REQUIRES_EQUIPMENT max=1,042,936 — 명백한 이상치. P95 기반 필터링 필요.

### 0.5 기타 현황

| 항목             |      값      |
| ---------------- | :----------: |
| 빈 이름 엔티티   |     0건      |
| NFKC 정규화 대상 |    753건     |
| confidence < 0.9 | 266건 (1.0%) |
| 음수 수량        |     0건      |

---

## 1. 전체 처리 흐름

```
merged_entities.json (25,708 엔티티)
    │
    ▼
[Phase A] 문자열 정규화
    │  · NFKC 유니코드 정규화 (753건)
    │  · 공백 통일 (435그룹)
    │  · LABOR_NORMALIZE_MAP 적용
    │  · 단위 표기 통일
    │
    ▼
[Phase B] 규칙 기반 중복 제거
    │  · (type, normalized_name, spec) 기준 그룹화 ← Codex #2 반영
    │  · 대표 엔티티 선정 + source_chunk_ids 집계
    │  · 예상: 25,708 → ~15,171
    │
    ▼
[Phase C] 관계 방향 보정
    │  · 725건 방향 오류 수정
    │  · 보정 불가 시 관계 삭제 + warnings
    │
    ▼
[Phase D] 이상치 필터링
    │  · 수량: P95×3 초과 → confidence 하향
    │  · quantity=0 관계 → 제거
    │
    ▼
[Phase E] 관계 참조 갱신
    │  · (type, name) 타입 안전 키로 관계 source/target 갱신 ← Codex #3 반영
    │  · 중복 관계 제거: (src, tgt, type, qty, unit) 기준 ← Codex #4 반영
    │  · 참조 무결성 검증
    │
    ▼
[Phase F] 엔티티 ID 부여
    │  · W-0001, L-0001 형식 글로벌 유니크 ID
    │
    ▼
normalized_entities.json (~15,171 엔티티)
```

---

## 2. Phase별 상세 설계

### Phase A. 문자열 정규화

```python
import unicodedata, re

UNIT_NORMALIZE = {
    "㎥": "m³", "㎡": "m²", "㎝": "cm", "㎜": "mm",
    "㎞": "km", "㏊": "ha", "ℓ": "L",
    "톤": "ton", "t": "ton",
    "키로와트": "kW", "KW": "kW",
    "시간": "hr", "시": "hr",
}

def normalize_name(name: str, entity_type: str) -> str:
    """
    단계별 정규화:
    
    1. NFKC 유니코드 정규화
       "㎥" → "m³", "㎡" → "m²"
       대상: 753건
       
    2. LABOR_NORMALIZE_MAP 적용 (config.py에 정의)
       "보 통 인 부" → "보통인부"
       "콘 크 리 트 공" → "콘크리트공"
       대상: Labor 타입만
       
    3. 공백 정규화
       연속 공백 → 단일 공백
       전후 공백 제거
       괄호 앞뒤 공백 제거: "굴착기 ( 0.6 )" → "굴착기(0.6)"
       대상: 435그룹
       
    4. 단위 표기 통일
       "㎥" ↔ "m³", "톤" ↔ "ton"
    """
```

**Labor 전용 확장 매핑** (config.py의 LABOR_NORMALIZE_MAP에 추가):

```python
# 분석에서 발견된 추가 변형
LABOR_NORMALIZE_MAP_EXT = {
    "중 급 기 술 자": "중급기술자",
    "고 급 기 술 자": "고급기술자",
    "초 급 기 술 자": "초급기술자",
    "중 급 기 능 사": "중급기능사",
    "초 급 기 능 사": "초급기능사",
}
```

### Phase B. 규칙 기반 중복 제거

```python
EntityKey = tuple[str, ...]  # 타입별 가변 길이 튜플
NameMap = dict[tuple[str, str], str]  # (type, old_name) → new_name

def deduplicate_entities(
    all_entities: list[dict],   # 모든 청크의 엔티티 플랫 리스트
) -> tuple[list[dict], NameMap]:
    """
    Returns:
      - deduped: 중복 제거된 대표 엔티티 리스트
      - name_map: {(type, 원래이름) → 대표이름} 매핑 (관계 갱신용)
                  ← Codex #3 반영: 타입 안전 키로 변경
    """
```

**그룹핑 키 설계** (⚠️ Codex #2 반영: spec 포함):

| 엔티티 유형 | 유니크 키                                    | 이유                            | 실측 고유 수 |
| ----------- | -------------------------------------------- | ------------------------------- | :----------: |
| WorkType    | `(type, normalized_name, spec)`              | 같은 공종도 규격별 다른 투입량  |    4,552     |
| Labor       | `(type, normalized_name)`                    | "보통인부"는 어디서든 동일      |     337      |
| Equipment   | `(type, normalized_name, spec)`              | "굴착기 0.4m³" ≠ "굴착기 0.7m³" |    1,820     |
| Material    | `(type, normalized_name, spec)`              | "철근 D13" ≠ "철근 D16"         |    1,684     |
| Note        | `(type, normalized_name, source_section_id)` | 섹션별로 다른 조건/주석         |    5,212     |
| Standard    | `(type, normalized_name)`                    | 기준은 유일                     |     569      |
| Section     | `(type, code)`                               | section_id가 유니크 키          |     997      |

> **Codex #2 데이터 근거**: WorkType 이름키 3,790 vs spec포함키 4,552 (차이 762), Equipment 1,197 vs 1,820 (차이 623), Material 1,293 vs 1,684 (차이 391). spec 미포함 시 합계 1,782건 과잉 병합 위험.

**spec 정규화 규칙**:

```python
def normalize_spec(spec: str | None) -> str:
    """
    spec을 정규화하여 동일 규격을 하나로 통일.
    None / "" / "없음" / "-" → ""  (빈 문자열로 통일)
    공백/괄호 정규화 적용
    NFKC 유니코드 정규화 적용
    """
```

**대표 엔티티 선정 규칙**:

```python
def pick_representative(group: list[dict]) -> dict:
    """
    같은 키의 엔티티 그룹에서 대표 1개 선정.
    
    1. confidence 최대값인 엔티티 선택
    2. 동점이면 source_method == "merged" > "llm" > "table_rule"
    3. 모든 source_chunk_ids 수집 (출처 추적 보존)
    4. spec: 가장 길고 구체적인 값 채택
    5. quantity: 평균이 아닌 최초 출처 값 유지 (맥락별로 다를 수 있음)
    """
```

**예상 결과**:

| 유형      |   Before   |    After    |    감소     |
| --------- | :--------: | :---------: | :---------: |
| WorkType  |   5,995    |   ~4,552    |   -1,443    |
| Labor     |   4,270    |    ~337     | **-3,933**  |
| Equipment |   2,706    |   ~1,820    |    -886     |
| Material  |   2,618    |   ~1,684    |    -934     |
| Note      |   6,912    |   ~5,212    |   -1,700    |
| Section   |   2,105    |    ~997     |   -1,108    |
| Standard  |   1,102    |    ~569     |    -533     |
| **합계**  | **25,708** | **~15,171** | **-10,537** |

### Phase C. 관계 방향 보정

```python
def fix_relation_directions(
    relationships: list[dict],
    entity_map: dict[tuple[str, str], dict],  # (type, name) → entity
    chunk_extractions: list[dict],  # 원본 청크 (WorkType 탐색용)
) -> tuple[list[dict], list[dict]]:
    """
    Returns:
      - fixed_rels: 보정된 관계 리스트
      - warnings: 보정 불가 관계 리스트
    """
```

**보정 전략 (패턴별)**:

#### 패턴 1: Equipment→X (REQUIRES_LABOR / USES_MATERIAL) — 447건

```python
# Equipment가 source인 경우, 같은 청크의 WorkType을 찾아 교체
# 예: Equipment("크레인") →REQUIRES_LABOR→ Labor("보통인부")
#  → WorkType("철골 세우기") →REQUIRES_LABOR→ Labor("보통인부")

def find_worktype_for_equipment(equip_name, chunk_id, extractions):
    """같은 청크에서 이 Equipment를 사용하는 WorkType 탐색.
    
    1차: REQUIRES_EQUIPMENT 관계에서 source가 WorkType인 것 검색
    2차: 같은 청크의 첫 번째 WorkType 사용 (fallback)
    3차: 못 찾으면 None → warnings에 기록
    """
```

#### 패턴 2: Equipment→Equipment (REQUIRES_EQUIPMENT) — 93건

```python
# 첫 번째 Equipment를 WorkType으로 재분류 시도
# 예: Equipment("콘크리트 타설") →REQUIRES_EQUIPMENT→ Equipment("레미콘")
#  → 정상: WorkType("콘크리트 타설") →REQUIRES_EQUIPMENT→ Equipment("레미콘")
#
# 판단 기준: 첫 번째의 이름이 "~타설", "~설치", "~가공" 등 공종 패턴이면 재분류
# 아니면: 관계 삭제 + warnings
```

#### 패턴 3: WorkType→WorkType (HAS_NOTE) — 64건

```python
# target의 이름이 주석 패턴이면 Note로 재분류
# 주석 패턴: "[주]", "①", "※", "별도 계상", "포함", "제외"
# 아니면: 관계 삭제
```

#### 패턴 4: Note→Note (HAS_NOTE) — 33건

```python
# 자기참조 무의미 → 관계 삭제
```

#### 패턴 5: 기타 (Material→Equipment 등) — 88건

```python
# 같은 청크의 WorkType을 source로 교체 시도
# 실패 시 관계 삭제
```

### Phase D. 이상치 필터링

```python
def filter_outliers(
    relationships: list[dict],
) -> tuple[list[dict], list[dict]]:
    """
    Returns:
      - filtered: 보정된 관계 리스트
      - outlier_log: 이상치 기록
    """
```

**필터링 규칙**:

| 관계 유형          |  임계값 (P95×3)  | 대상  | 처리                             |
| ------------------ | :--------------: | :---: | -------------------------------- |
| REQUIRES_LABOR     |  quantity > 75   | 101건 | confidence → 0.3, flag="outlier" |
| REQUIRES_EQUIPMENT | quantity > 3,300 | 39건  | 동일                             |
| USES_MATERIAL      |  quantity > 225  | 37건  | 동일                             |
| 모든 유형          |  quantity == 0   |   —   | 관계 삭제 (무의미)               |

> **왜 삭제하지 않는가**: 이상치라도 원본에 실제 존재하는 수치일 수 있음. confidence를 낮추고 flag를 달아 검색 시 하위 우선순위로 처리.

### Phase E. 관계 참조 갱신

```python
def update_relationship_references(
    relationships: list[dict],
    name_map: dict[tuple[str, str], str],  # ← Codex #3: (type, old_name) → new_name
    valid_entities: set[tuple[str, str]],  # ← (type, name) 쌍으로 유효성 검증
) -> tuple[list[dict], int]:
    """
    1. source/target 이름을 name_map으로 갱신
       - 키: (source_type, source_name) → 대표 이름
       - ← Codex #3 반영: 동일 이름 다중 타입 870건 오매핑 방지
    2. 갱신 후 (type, name)이 valid_entities에 없으면 관계 삭제
    3. 중복 관계 제거:
       - 키: (source, target, type, quantity, unit, per_unit)
       - ← Codex #4 반영: payload 상이 그룹 2,093건 보존
       - quantity/unit이 같으면 합침, 다르면 별도 유지
       - source_chunk_ids만 병합
    
    Returns: (updated_rels, removed_count)
    """
```

> **Codex #3 데이터 근거**: 동일 이름 다중 타입 870건. 예: "콘크리트" → WorkType? Material?  
> **Codex #4 데이터 근거**: 중복 그룹 2,104개 중 payload 상이 2,093개. 단순 (src, tgt, type) 기준 시 수량/근거 정보 손실.

### Phase F. 엔티티 ID 부여

```python
TYPE_PREFIX = {
    "WorkType": "W", "Labor": "L", "Equipment": "E",
    "Material": "M", "Section": "S", "Note": "N", "Standard": "ST",
}

def assign_entity_ids(entities: list[dict]) -> dict[tuple[str, str], str]:
    """
    글로벌 유니크 ID 부여.
    
    ID 형식: {prefix}-{4자리순번}
    예: W-0001, L-0001, E-0001, ...
    
    Returns: {(type, entity_name) → entity_id} 매핑
             ← Codex #3 반영: 타입 안전 키 사용
    """
```

---

## 3. 출력 파일 구조

```json
// normalized_entities.json
{
  "total_entities": 15171,
  "total_relationships": 28000,
  "entity_type_counts": { ... },
  "relationship_type_counts": { ... },
  
  "normalization_stats": {
    "input_entities": 25708,
    "output_entities": 15171,
    "dedup_removed": 10537,
    "direction_fixed": 650,
    "direction_deleted": 75,
    "outliers_flagged": 177,
    "unicode_normalized": 753,
    "space_normalized": 435
  },
  
  "entities": [
    {
      "entity_id": "W-0001",
      "type": "WorkType",
      "name": "콘크리트 타설",
      "normalized_name": "콘크리트타설",
      "spec": "보통콘크리트",
      "unit": "m³",
      "quantity": null,
      "properties": {},
      "confidence": 0.95,
      "source_chunk_ids": ["C-0100", "C-0101", "C-0102"],
      "source_section_id": "6-3-1",
      "department": "토목부문",
      "source_method": "merged"
    }
  ],
  
  "relationships": [
    {
      "source_entity_id": "W-0001",
      "target_entity_id": "L-0005",
      "type": "REQUIRES_LABOR",
      "quantity": 0.67,
      "unit": "인",
      "per_unit": "1m³당",
      "properties": {},
      "source_chunk_ids": ["C-0100", "C-0101"]
    }
  ],
  
  "global_relationships": {
    "HAS_CHILD": [ ... ],
    "REFERENCES": [ ... ]
  },
  
  "warnings": [
    {"type": "direction_delete", "chunk_id": "C-0050", "detail": "..."},
    {"type": "outlier", "chunk_id": "C-0200", "detail": "..."}
  ]
}
```

---

## 4. 검증 항목

|   #    | 검증             |         기준          | 방법                                              |
| :----: | ---------------- | :-------------------: | ------------------------------------------------- |
| **N1** | 정규화 감소율    |        30~60%         | `(before - after) / before`                       |
| **N2** | 관계 방향 오류   |          0건          | 보정 후 전수 재검사                               |
| **N3** | 수량 이상치      |     모두 flagged      | outlier_log 전수 확인                             |
| **N4** | 빈 이름 엔티티   |          0건          | 전수 검사                                         |
| **N5** | 참조 무결성      | (type,name) 100% 유효 | 타입 안전 키 기반 entity_id 존재 검사             |
| **N6** | 중복 관계        |          0건          | (src, tgt, type, qty, unit, per_unit) 유니크 검사 |
| **N7** | entity_id 유니크 |         100%          | 전수 검사                                         |

---

## 5. 구현 순서 & 예상 시간

|   순서   | 작업                         |    예상 시간    | 산출물                    |
| :------: | ---------------------------- | :-------------: | ------------------------- |
|    1     | Phase A: 문자열 정규화       |      20분       | normalize_name()          |
|    2     | Phase B: 규칙 기반 중복 제거 |      35분       | dedup + name_map          |
|    3     | Phase C: 관계 방향 보정      |      30분       | fix_relation_directions() |
|    4     | Phase D: 이상치 필터링       |      15분       | filter_outliers()         |
|    5     | Phase E: 관계 참조 갱신      |      20분       | update_references()       |
|    6     | Phase F: 엔티티 ID 부여      |      10분       | assign_ids()              |
|    7     | 검증 (N1~N7)                 |      20분       | verify_step4.py           |
| **합계** |                              | **~2시간 30분** |                           |

---

## 6. 리스크 & 대응

|   #   | 리스크                                          | 가능성 | 영향  | 대응                                                    |
| :---: | ----------------------------------------------- | :----: | :---: | ------------------------------------------------------- |
|   1   | 과잉 중복 제거 (다른 엔티티를 같은 것으로 합침) |  낮음  | 높음  | `(type, normalized_name, spec)` 포함 키 사용 (Codex #2) |
|   2   | name_map 타입 충돌 (동일 이름 다중 타입)        |  낮음  | 높음  | `(type, name)` 타입 안전 키 사용 (Codex #3)             |
|   3   | 관계 중복 제거 시 payload 정보 손실             |  낮음  | 중간  | `(src, tgt, type, qty, unit)` 확장 키 사용 (Codex #4)   |
|   4   | 방향 보정 시 잘못된 WorkType 매칭               |  낮음  | 중간  | 같은 청크 내에서만 탐색. 못 찾으면 삭제                 |
|   5   | 이상치가 실제 유효 수치                         |  중간  | 낮음  | 삭제하지 않고 confidence만 하향. flag로 추적            |
|   6   | Note 정규화 어려움 (자유 텍스트)                |  높음  | 낮음  | 섹션별 별도 관리 (source_section_id 포함 키)            |

---

## 7. Codex 실행타당성검토 대응 현황

|   #   | Codex 지적                        |     판정      | 조치 내용                                         |
| :---: | --------------------------------- | :-----------: | ------------------------------------------------- |
|   1   | 목표 수치 충돌 (14,171 vs 15,171) |    ✅ 수용     | **~15,171**로 단일화 (§0.1, §1, §2, §3)           |
|   2   | dedup 키에 spec 미반영            |    ✅ 수용     | WorkType/Equipment/Material 키에 spec 포함 (§2-B) |
|   3   | name_map 타입 충돌 위험           |    ✅ 수용     | `dict[tuple[str,str], str]`로 변경 (§2-B,E,F)     |
|   4   | 관계 dedup 키 단순                | ✅ 조건부 수용 | `(src, tgt, type, qty, unit)` 확장 (§2-E)         |
|   5   | Section 예상치 불일치             |    ✅ 수용     | 997(실측)로 보정 (§0.1, §2-B)                     |
|   6   | step4_normalizer.py 미존재        |    ✅ 인지     | 구현 착수 시 신규 생성 (계획서이므로 정상)        |

---

## 8. 의존 파일

| 파일                                 | 용도                             |
| ------------------------------------ | -------------------------------- |
| `phase2_output/merged_entities.json` | **입력** (Step 2.3 출력)         |
| `phase2_extraction/config.py`        | LABOR_NORMALIZE_MAP, UNIT 키워드 |
| `phase2_extraction/schemas.py`       | Entity, Relationship 모델 참조   |
| `phase1_output/chunks.json`          | 원본 텍스트 대조 (방향 보정 시)  |



