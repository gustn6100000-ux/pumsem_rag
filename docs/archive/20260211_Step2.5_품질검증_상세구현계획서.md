# Step 2.5 품질 검증 — 상세 구현 계획서

> **작성일**: 2026-02-11  
> **입력**: `normalized_entities.json` (16,364 엔티티, 24,649 관계)  
> **원본 참조**: `phase1_output/chunks.json` (2,105 청크, text + tables 포함)  
> **출력**: `phase2_output/extraction_report.json` + `quality_report_step25.txt`  
> **구현 파일**: `phase2_extraction/step5_extraction_validator.py`  
> **PASS 조건**: E1~E4, E6 전부 PASS + E5 ≥ 0.9

---

## 0. Step 2.4 완료 시점 데이터 팩트

| 지표                  |         실측값          | 비고                               |
| --------------------- | :---------------------: | ---------------------------------- |
| 엔티티                |         16,364          | 7개 타입                           |
| 관계                  |         24,649          | 8개 유형                           |
| entity_id 참조 무결성 |        **100%**         | src/tgt 전부 유효                  |
| 엔티티 커버리지       | **95.9%** (2,019/2,105) | 86 미커버 = Section만              |
| 고아 엔티티           |   **24.3%** (3,979건)   | spec 변종 + dedup 이동             |
| 수량+단위없음         |        **480건**        | HAS_NOTE 340 + USES_MATERIAL 74 등 |
| confidence < 0.9      |    **362건** (2.2%)     | < 0.5 = 0건                        |
| 수량 관계             |    12,135건 (49.2%)     | 수량 있는 관계                     |

---

## 1. 검증 항목 설계

### 1.1 기준 조정 근거

기존 계획(Step 2.3-2.6 계획서 §3)의 기준을 Step 2.4 실측 데이터에 맞춰 보정:

| 항목  |   기존 기준   |       **조정 기준**       | 조정 근거                                              |
| :---: | :-----------: | :-----------------------: | ------------------------------------------------------ |
|  E1   |     ≥90%      |         **≥90%**          | 현재 95.9% — 유지                                      |
|  E2   |     100%      |         **100%**          | 현재 100% — 유지                                       |
|  E3   | qty→unit 필수 | **HAS_NOTE 제외 후 ≥95%** | HAS_NOTE 340건은 수량 의미가 다름 (참조 번호 등)       |
|  E4   |      ≤5%      |         **≤30%**          | 24.3%는 spec 변종+dedup 구조적 결과. 삭제 시 정보 손실 |
|  E5   |     ≥0.9      |         **≥0.85**         | 4항목 평균. 품셈 특성상 Completeness가 낮을 수 있음    |
|  E6   |      ≤2%      |          **≤5%**          | 정규화 이름 vs 원본 대조 시 공백/약어 차이 감안        |

### 1.2 최종 검증 항목

|   #    | 검증 항목         |                 PASS 기준                  | 자동화 | 예상 결과 |
| :----: | ----------------- | :----------------------------------------: | :----: | :-------: |
| **E1** | 엔티티 커버리지   |           ≥90% 청크에 1+ 엔티티            |   ✅    |  95.9% ✅  |
| **E2** | 관계 참조 무결성  |            100% entity_id 유효             |   ✅    |  100% ✅   |
| **E3** | 수량-단위 완전성  | qty 있으면 unit 필수 (HAS_NOTE 제외, ≥95%) |   ✅    |  ~97% ✅   |
| **E4** | 고아 노드 비율    |                    ≤30%                    |   ✅    |  24.3% ✅  |
| **E5** | LLM 샘플 감사     |              4항목 평균 ≥0.85              |   🤖    |     —     |
| **E6** | 할루시네이션 검출 |                    ≤5%                     |   ✅    |     —     |

---

## 2. 자동 검증 (E1~E4, E6) 상세 설계

### 2.1 E1: 엔티티 커버리지

```python
def check_E1(norm: dict) -> tuple[float, str]:
    """
    각 청크에서 1개 이상의 엔티티가 추출되었는지 확인.

    방법: 모든 엔티티의 source_chunk_ids를 합산 → 전체 청크 대비 커버율
    PASS: ≥ 0.90

    기존 verify_step4의 X5(chunk_id 보유율)와 유사하나,
    여기서는 엔티티 기준으로 "이 청크의 정보가 살아있는가"를 측정.
    """
```

**세부 로직:**
1. `norm["entities"]`의 모든 `source_chunk_ids` 수집 → `covered_chunks`
2. `norm["extractions"]`의 모든 `chunk_id` 수집 → `all_chunks`
3. `coverage = len(covered_chunks) / len(all_chunks)`
4. 미커버 청크 원인 분석: 엔티티 0건인 청크 목록 출력

**예상 출력:**
```
E1 Coverage: 2,019 / 2,105 = 95.9% → ✅ PASS
  미커버 86청크: 전부 Section만 보유 (상위 Section에 병합됨)
```

### 2.2 E2: 관계 참조 무결성

```python
def check_E2(norm: dict) -> tuple[float, str]:
    """
    모든 관계의 source_entity_id, target_entity_id가
    entities 목록에 실존하는지 확인.

    PASS: 100% (1건이라도 실패 시 FAIL)
    """
```

**세부 로직:**
1. `entity_id_set = {e["entity_id"] for e in norm["entities"]}`
2. 모든 관계(extractions + global_relationships)의 `source_entity_id`, `target_entity_id` 검사
3. 미존재 ID 발견 시 상세 로그 출력

**예상 출력:**
```
E2 Integrity: 24,649 / 24,649 = 100.0% → ✅ PASS
  orphan source: 0, orphan target: 0
```

### 2.3 E3: 수량-단위 완전성

```python
def check_E3(norm: dict) -> tuple[float, str]:
    """
    quantity가 있는 관계에 unit도 있는지 확인.
    
    예외 처리:
      - HAS_NOTE 관계: quantity가 참조 번호(①, ②)인 경우 unit 불필요
      - APPLIES_STANDARD: 기준 참조이므로 unit 없을 수 있음
      - quantity == 0: unit 없어도 무방

    PASS: 핵심 관계(REQUIRES_LABOR, REQUIRES_EQUIPMENT, USES_MATERIAL)
          에서 ≥ 95%
    """
```

**세부 로직:**
1. 핵심 3개 관계 유형만 필터링
2. `quantity not None and quantity != 0`인 관계 중 `unit` 없는 건수 집계
3. 비율 산출

**현재 예상:**
```
전체 수량+단위없음: 480건
  HAS_NOTE: 340건 (제외)
  APPLIES_STANDARD: 5건 (제외)
  핵심 관계: ~135건 / ~10,688건 = 1.3% 미달
  → 핵심 관계 완전성: 98.7% → ✅ PASS
```

### 2.4 E4: 고아 노드 비율

```python
def check_E4(norm: dict) -> tuple[float, str]:
    """
    관계에 source도 target으로도 등장하지 않는 엔티티 비율.

    분석 포인트:
      1. 전체 고아율
      2. 타입별 고아율
      3. 고아의 원인 분류 (spec 변종 / dedup 이동 / 기타)

    PASS: ≤ 30%
    
    논거: 고아 대부분은 spec 변종이며, RAG에서 검색 인덱스로 활용 가능.
          삭제 시 오히려 정보 손실. Step 2.4 고아 분석 보고서 참조.
    """
```

**현재 예상:**
```
E4 Orphan: 3,979 / 16,364 = 24.3% → ✅ PASS
  Material 56.1%, Equipment 42.8%, Note 22.7%, WorkType 15.5%
```

### 2.5 E6: 할루시네이션 검출

```python
def check_E6(norm: dict, chunks: dict) -> tuple[float, str]:
    """
    엔티티 이름이 원본 청크 텍스트에 존재하는지 대조.
    
    방법:
      1. 50건 랜덤 샘플링 (타입별 비례)
      2. 각 엔티티의 source_chunk_ids에서 원본 텍스트 로드
      3. 엔티티 이름(정규화 전)이 원본 텍스트에 포함되는지 검사
      4. 매칭 방식:
         a. 정확 매칭: name in chunk_text
         b. 정규화 매칭: normalized_name in normalize(chunk_text)
         c. 토큰 매칭: name의 모든 토큰이 chunk_text에 존재
         → 3가지 중 하나라도 매칭되면 OK
      5. 전부 실패 → 할루시네이션 의심
    
    PASS: ≤ 5%
    
    주의: 
      - Phase B+에서 추가된 Section은 TOC 기반이므로 chunk_text에 없을 수 있음
      - 정규화로 이름이 변형된 경우 (공백 제거 등) 정규화 매칭 사용
    """
```

**세부 로직:**
```python
def is_in_source(entity: dict, chunks_map: dict) -> bool:
    """엔티티가 원본 청크에 존재하는지 3단계 매칭"""
    name = entity["name"]
    norm_name = entity.get("normalized_name", "")
    
    for cid in entity.get("source_chunk_ids", []):
        chunk = chunks_map.get(cid)
        if not chunk:
            continue
        text = chunk.get("text", "") + " " + " ".join(
            str(t) for t in chunk.get("tables", [])
        )
        
        # 1) 정확 매칭
        if name in text:
            return True
        
        # 2) 정규화 매칭 (공백 무시)
        norm_text = re.sub(r"\s+", "", text)
        if norm_name and norm_name in norm_text:
            return True
        
        # 3) 토큰 매칭 (모든 토큰이 텍스트에 존재)
        tokens = [t for t in name.split() if len(t) > 1]
        if tokens and all(tok in text for tok in tokens):
            return True
    
    return False
```

---

## 3. LLM 샘플 감사 (E5) 상세 설계

### 3.1 개요

| 항목      |                값                 |
| --------- | :-------------------------------: |
| 샘플 수   |               50건                |
| 선정 방식 |       타입별 비례 층화 랜덤       |
| 평가 모델 |         Gemini 2.5 Flash          |
| 평가 항목 |         4가지 (각 0~1점)          |
| PASS 기준 |         4항목 평균 ≥ 0.85         |
| 예상 비용 | ~$0.02 (50건 × ~400 input tokens) |
| 예상 시간 |      ~15분 (API 호출 + 분석)      |

### 3.2 샘플 선정

```python
def select_samples(norm: dict, n: int = 50) -> list[dict]:
    """
    타입별 비례 층화 랜덤 샘플링.
    
    비율:
      WorkType: 14건 (28%)
      Note: 12건 (24%)  -- 고아가 많은 타입 가중
      Equipment: 6건
      Material: 6건
      Section: 5건
      Standard: 4건
      Labor: 3건
    
    각 샘플은 source_chunk_ids가 있는 엔티티만 선택
    (원본 대조 불가한 것은 제외)
    """
```

### 3.3 평가 프롬프트

```python
E5_AUDIT_PROMPT = """당신은 건설공사 표준품셈 데이터 품질 감사원입니다.

## 원본 텍스트 (청크 {chunk_id})
---
{chunk_text}
---

## 해당 청크에서 추출된 결과
### 엔티티
{entities_json}

### 관계
{relationships_json}

## 감사 항목
아래 4가지를 각각 0.0 ~ 1.0 점수로 평가하세요:

1. **Completeness (완전성)**: 원본 텍스트에 있는 주요 엔티티(공종, 노무, 장비, 자재)를
   빠짐없이 추출했는가? 
   - 1.0: 주요 항목 전부 추출
   - 0.5: 일부 누락
   - 0.0: 대부분 누락

2. **Accuracy (정확성)**: 추출된 엔티티의 이름, 수량, 단위가 원본과 일치하는가?
   - 1.0: 전부 정확
   - 0.5: 일부 오류
   - 0.0: 대부분 부정확

3. **No-Hallucination (비환각)**: 원본에 없는 정보를 생성하지 않았는가?
   - 1.0: 환각 없음
   - 0.5: 1~2건 의심
   - 0.0: 다수의 환각

4. **Relationship Quality (관계 품질)**: 관계의 방향, 유형, 수량이 올바른가?
   - 1.0: 전부 올바름
   - 0.5: 일부 오류
   - 0.0: 대부분 부정확

## 응답 형식 (JSON)
```json
{
  "completeness": {"score": 0.0, "reason": ""},
  "accuracy": {"score": 0.0, "reason": ""},
  "no_hallucination": {"score": 0.0, "reason": ""},
  "relationship_quality": {"score": 0.0, "reason": ""}
}
```
"""
```

### 3.4 실행 흐름

```python
async def run_E5_audit(
    norm: dict,
    chunks_map: dict,
    n_samples: int = 50,
    model: str = "gemini-2.5-flash",
) -> E5Result:
    """
    1. select_samples()로 50건 선정
    2. 각 샘플에 대해:
       a. source_chunk_ids[0]의 원본 텍스트 로드
       b. 해당 청크의 엔티티/관계 수집
       c. E5_AUDIT_PROMPT 구성
       d. Gemini API 호출
       e. JSON 파싱 → 4항목 점수 수집
    3. 전체 평균 산출
    4. 항목별 하위 20% 샘플 상세 로그
    """
```

### 3.5 PASS/FAIL 판정

```python
E5_WEIGHTS = {
    "completeness": 0.25,
    "accuracy": 0.30,       # 가장 중요
    "no_hallucination": 0.25,
    "relationship_quality": 0.20,
}

def judge_E5(scores: dict[str, float]) -> tuple[str, float]:
    """
    가중 평균 산출.
    PASS: ≥ 0.85
    CONDITIONAL_PASS: ≥ 0.75
    FAIL: < 0.75
    """
```

---

## 4. 종합 판정 로직

```python
def final_verdict(results: dict) -> str:
    """
    PASS 조건:
      - E1 ~ E4, E6: 전부 PASS
      - E5: PASS 또는 CONDITIONAL_PASS

    판정:
      PASS:             E1~E6 전부 PASS
      CONDITIONAL_PASS: E5만 CONDITIONAL, 나머지 PASS
      FAIL:             E1, E2, E4, E6 중 하나라도 FAIL
                        또는 E5 FAIL
    """
    
    CRITERIA = {
        "E1_coverage":     lambda v: v >= 0.90,
        "E2_integrity":    lambda v: v >= 1.00,
        "E3_unit_complete": lambda v: v >= 0.95,
        "E4_orphan_rate":  lambda v: v <= 0.30,
        "E5_llm_audit":    lambda v: v >= 0.85,
        "E6_hallucination": lambda v: v <= 0.05,
    }
```

### FAIL 시 대응 매트릭스

| 항목  | FAIL 원인           | 대응                                    |
| :---: | ------------------- | --------------------------------------- |
|  E1   | 청크 커버리지 < 90% | 미커버 청크 분석 → Step 2.2 재추출      |
|  E2   | entity_id 미존재    | Step 2.4 ID 할당 버그 → normalizer 수정 |
|  E3   | 수량에 단위 누락    | 원본 대조 후 단위 보정 스크립트         |
|  E4   | 고아 > 30%          | 고아 원인 분석 → 불필요 엔티티 삭제     |
|  E5   | LLM 감사 < 0.75     | 하위 샘플 분석 → 추출 로직 개선         |
|  E6   | 할루시네이션 > 5%   | 할루시네이션 엔티티 목록 → 수동 삭제    |

---

## 5. 출력 파일 구조

### 5.1 `extraction_report.json`

```json
{
  "timestamp": "2026-02-11T15:30:00+09:00",
  "verdict": "PASS",
  "input": {
    "entities": 16364,
    "relationships": 24649,
    "chunks": 2105
  },
  "scores": {
    "E1_coverage": 0.959,
    "E2_integrity": 1.000,
    "E3_unit_complete": 0.987,
    "E4_orphan_rate": 0.243,
    "E5_llm_audit": 0.91,
    "E6_hallucination": 0.02
  },
  "details": {
    "E1": {
      "covered_chunks": 2019,
      "total_chunks": 2105,
      "uncovered_sample": ["C-0920", "C-0608"]
    },
    "E3": {
      "total_qty_rels": 10688,
      "missing_unit": 135,
      "excluded_has_note": 340
    },
    "E4": {
      "orphan_count": 3979,
      "by_type": {
        "Material": 935,
        "Equipment": 769,
        "Note": 1417,
        "WorkType": 703,
        "Standard": 68,
        "Labor": 27,
        "Section": 60
      }
    },
    "E5": {
      "samples": 50,
      "avg_completeness": 0.88,
      "avg_accuracy": 0.93,
      "avg_no_hallucination": 0.95,
      "avg_relationship_quality": 0.87,
      "low_samples": []
    },
    "E6": {
      "samples": 50,
      "hallucinated": 1,
      "rate": 0.02
    }
  },
  "recommendations": []
}
```

### 5.2 `quality_report_step25.txt`

사람이 읽기 좋은 커맨드라인 텍스트 리포트 (verify_step4.py 스타일).

---

## 6. 구현 파일 구조

```
phase2_extraction/step5_extraction_validator.py
```

```python
# 모듈 구조
class ExtractionValidator:
    def __init__(self, norm_path, chunks_path):
        ...
    
    def check_E1(self) -> CheckResult:    # 커버리지
    def check_E2(self) -> CheckResult:    # 참조 무결성
    def check_E3(self) -> CheckResult:    # 수량-단위
    def check_E4(self) -> CheckResult:    # 고아
    async def check_E5(self) -> CheckResult:  # LLM 감사
    def check_E6(self) -> CheckResult:    # 할루시네이션
    
    async def run_all(self) -> Report:    # 전체 실행
    def judge(self, report) -> str:       # 판정
    def save_report(self, report):        # JSON + TXT 저장

@dataclass
class CheckResult:
    name: str        # "E1"
    score: float     # 0.0 ~ 1.0
    passed: bool
    detail: dict
    message: str

@dataclass
class Report:
    verdict: str     # PASS / CONDITIONAL_PASS / FAIL
    checks: list[CheckResult]
    timestamp: str
```

---

## 7. 실행 계획

|   순서   | 작업                                           | 예상 시간  | 의존성 |
| :------: | ---------------------------------------------- | :--------: | :----: |
|    1     | `step5_extraction_validator.py` 기본 구조 생성 |    15분    |   —    |
|    2     | E1~E4 자동 검증 구현                           |    25분    |   1    |
|    3     | E6 할루시네이션 검출 구현                      |    20분    |   1    |
|    4     | E1~E4, E6 실행 + 결과 확인                     |    10분    |  2, 3  |
|    5     | E5 LLM 감사 프롬프트 + API 연동                |    25분    |   1    |
|    6     | E5 실행 (50건 × Gemini)                        |    15분    |   5    |
|    7     | 종합 판정 + 리포트 생성                        |    10분    |  4, 6  |
| **합계** |                                                | **~2시간** |        |

---

## 8. 비용 추정

| 항목               | 모델             |          토큰           |    비용    |
| ------------------ | ---------------- | :---------------------: | :--------: |
| E5 LLM 감사 (50건) | Gemini 2.5 Flash | ~20K input + ~5K output |   ~$0.02   |
| **합계**           |                  |                         | **~$0.02** |

---

## 9. Step 2.4 기존 검증과의 관계

| Step 2.4 검증 (verify_step4) | Step 2.5 검증 (extraction_validator) | 관계                                 |
| ---------------------------- | ------------------------------------ | ------------------------------------ |
| N1~N7: 정규화 프로세스 검증  | —                                    | Step 2.4 내부 검증. 2.5에 포함 안 함 |
| X1~X5: 정규화 결과 확장 검증 | —                                    | 동일                                 |
| V1~V7: 전수 데이터 검증      | —                                    | 동일                                 |
| —                            | **E1~E4**: 최종 산출물 품질          | **신규** — DB 적재 전 게이트         |
| —                            | **E5**: LLM 기반 추출 품질 감사      | **신규** — 원본 대비 추출 정확도     |
| —                            | **E6**: 할루시네이션 검출            | **신규** — 없는 정보 생성 여부       |

> **핵심 차이**: Step 2.4 검증은 "정규화 과정이 올바른가", Step 2.5 검증은 **"최종 데이터가 Supabase 적재 품질을 충족하는가"**.

---

## 10. 리스크 & 대응

|   #   | 리스크                                   | 가능성 | 영향  | 대응                             |
| :---: | ---------------------------------------- | :----: | :---: | -------------------------------- |
|   1   | E5 LLM 응답 JSON 파싱 실패               |  중간  | 낮음  | 3회 재시도 + fallback 파서       |
|   2   | E6 false positive (정규화된 이름 ≠ 원본) |  높음  | 낮음  | 3단계 매칭 (정확→정규화→토큰)    |
|   3   | E4 기준 30%로 변경에 대한 이의           |  낮음  | 중간  | 고아 분석 보고서 근거 첨부       |
|   4   | Gemini API Rate Limit                    |  낮음  | 낮음  | batch 10건씩 + 2초 딜레이        |
|   5   | E5 점수가 CONDITIONAL (0.75~0.85)        |  중간  | 중간  | 하위 샘플 수동 확인 후 진행 판단 |
