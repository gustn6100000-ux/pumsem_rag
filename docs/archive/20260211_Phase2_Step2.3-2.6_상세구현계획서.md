# Phase 2 후속 작업 (Step 2.3 ~ 2.6) 상세 구현 계획서

> **문서 버전**: v1.0  
> **작성일**: 2026-02-11  
> **전제**: Step 2.1 (테이블 규칙 추출) + Step 2.2 (LLM 추출) 완료  
> **입력**: `table_entities.json` (8,655 엔티티) + `llm_entities.json` (20,973 엔티티)  
> **최종 출력**: Supabase `graph_entities` + `graph_relationships` + `graph_chunks` 테이블  

---

## 0. 현재 상태 요약

| 구분     | Step 2.1 (테이블) | Step 2.2 (LLM) |  **합계**  |
| -------- | :---------------: | :------------: | :--------: |
| 엔티티   |       8,655       |     20,973     | **29,628** |
| 관계     |       2,841       |     19,855     | **22,696** |
| 커버리지 |       82.7%       |     99.9%      |     —      |

### 알려진 이슈 (후속 Step에서 해결)

|   #   | 이슈                                             | 해당 Step |
| :---: | ------------------------------------------------ | :-------: |
|   1   | 관계 방향 오류 774건 (3.9%) — Equipment→Labor 등 | Step 2.4  |
|   2   | 고아 엔티티 1,943건 (9.3%) — 관계 미참여         | Step 2.3  |
|   3   | 노무 수량 이상치 63건 — max 646.45               | Step 2.4  |
|   4   | Note 합성 이름 707건 — LLM이 문맥에서 의미 합성  | Step 2.4  |
|   5   | BELONGS_TO 관계 부족 — Step 2.1에서 152건만      | Step 2.3  |
|   6   | REFERENCES 관계 0건 — cross_ref 미처리           | Step 2.3  |
|   7   | HAS_CHILD 관계 0건 — 섹션 계층 미생성            | Step 2.3  |
|   8   | Step 2.1 ↔ 2.2 중복 엔티티 존재                  | Step 2.3  |

---

## 1. Step 2.3: 관계 생성 & 병합

### 1.1 목적

Step 2.1(테이블)과 Step 2.2(LLM)의 결과를 **하나의 통합 그래프**로 병합하고, 메타데이터에서 자동 생성 가능한 **구조적 관계**(BELONGS_TO, HAS_CHILD, REFERENCES)를 추가한다.

### 1.2 구현 파일

```
phase2_extraction/step3_relation_builder.py
```

### 1.3 전체 처리 흐름

```
table_entities.json ─┐
                     ├─→ [A] 병합 → [B] BELONGS_TO → [C] HAS_CHILD → [D] REFERENCES → merged_entities.json
llm_entities.json ───┘       │              │               │
                        chunks.json    toc_parsed.json  cross_refs
```

### 1.4 세부 작업

#### A. 엔티티 & 관계 병합 (Merge)

**규칙**: 같은 `chunk_id`에서 Step 2.1과 Step 2.2가 동일 엔티티를 추출한 경우 하나로 통합.

```python
def merge_extractions(table_result: BatchResult, llm_result: BatchResult) -> BatchResult:
    """
    병합 전략:
    1. chunk_id 기준으로 양쪽 ExtractionResult 매칭
    2. 같은 chunk 내 엔티티 비교:
       a. normalized_name + type이 동일 → 병합 (LLM 결과 우선, 테이블 수량 보존)
       b. 다르면 → 양쪽 모두 유지
    3. 관계도 동일 로직으로 중복 제거
    4. Step 2.1에만 있는 청크(226개)의 결과도 포함
    """
```

**병합 우선순위**:
| 필드            |  우선순위   | 이유                             |
| --------------- | :---------: | -------------------------------- |
| `name`          |  LLM 우선   | LLM이 더 정확한 한국어 이름 생성 |
| `quantity`      | 테이블 우선 | 테이블에서 추출한 수치가 더 정확 |
| `unit`          | 테이블 우선 | 동일 이유                        |
| `spec`          |  LLM 우선   | LLM이 규격을 더 풍부하게 추출    |
| `confidence`    |   최대값    | 양쪽 중 높은 신뢰도 채택         |
| `source_method` | `"merged"`  | 병합 출처 표시                   |

**예상 결과**:
```
Before: 29,628 엔티티 (중복 포함)
After:  ~25,000 엔티티 (중복 제거 후)
```

#### B. BELONGS_TO 관계 생성

**목적**: 모든 WorkType 엔티티를 해당 Section에 연결.

```python
def generate_belongs_to(merged: BatchResult, chunks: list[dict]) -> list[Relationship]:
    """
    처리 로직:
    1. 각 ChunkExtraction의 section_id로 Section 엔티티 생성 (없으면 신규)
    2. 해당 청크의 모든 WorkType → Section 관계 생성
    3. section_id 검증: 
       - 정상 패턴 (x-y-z): 정상 처리
       - 비정상 패턴 (x-y, x-y#n): 최대한 매핑, 실패 시 chapter 기준 fallback
       - 빈값/unknown: department 기준 최상위 섹션에 연결
    """
```

**section_id 검증 규칙**:
```python
import re

def validate_section_id(sid: str) -> tuple[bool, str]:
    """
    Returns: (is_valid, normalized_id)
    
    정상: "6-3-1" → (True, "6-3-1")
    보정: "6-3#2" → (True, "6-3")     # '#' 이후 제거
    보정: "6-3"   → (True, "6-3")     # 2단계도 허용
    실패: ""      → (False, "unknown")
    """
    if not sid or sid.strip() == "":
        return False, "unknown"
    
    # '#' 이후 제거 (하위 분류 마커)
    base = sid.split("#")[0].strip()
    
    # x-y 또는 x-y-z 패턴 검증
    pattern = re.compile(r'^\d{1,2}-\d{1,2}(-\d{1,3})?$')
    if pattern.match(base):
        return True, base
    
    return False, "unknown"
```

**예상**: ~6,000개 BELONGS_TO 관계 신규 생성

#### C. HAS_CHILD 관계 생성 (섹션 계층)

```python
def generate_has_child(toc_data: dict) -> list[Relationship]:
    """
    toc_parsed.json의 section_map에서 섹션 계층 자동 생성.
    
    예시:
      "6"   → "6-1", "6-2", "6-3"       (장 → 절)
      "6-3" → "6-3-1", "6-3-2", "6-3-3" (절 → 항)
    
    처리:
    1. section_map의 모든 section_id를 레벨별 그룹화
    2. 상위 ID를 prefix로 공유하는 하위 ID 검색
    3. HAS_CHILD(parent, child) 관계 생성
    """
```

**예상**: ~1,100개 HAS_CHILD 관계

#### D. REFERENCES 관계 생성 (교차참조)

```python
def generate_references(chunks: list[dict]) -> list[Relationship]:
    """
    chunks.json의 cross_references 필드에서 섹션 간 참조 관계 생성.
    
    예시 입력:
      chunk.cross_references = [
        {"ref_section": "4-2-3", "context": "철근 가공 참조"}
      ]
    
    생성:
      REFERENCES(current_section, "4-2-3", context="철근 가공 참조")
    
    검증:
    1. ref_section이 실제 존재하는 section_id인지 확인
    2. 존재하지 않으면 warnings에 기록하고 스킵
    3. 양방향 참조인 경우 단방향 1개만 생성 (중복 방지)
    """
```

**예상**: ~200~500개 REFERENCES 관계

### 1.5 출력 파일

```
phase2_output/merged_entities.json (BatchResult 형식)
```

| 필드                                |         예상 값         |
| ----------------------------------- | :---------------------: |
| total_entities                      |         ~26,000         |
| total_relationships                 |         ~30,000         |
| entity_type_counts.Section          |      ~1,100 (신규)      |
| relationship_type_counts.BELONGS_TO | ~6,000 (신규 대폭 증가) |
| relationship_type_counts.HAS_CHILD  |      ~1,100 (신규)      |
| relationship_type_counts.REFERENCES |       ~300 (신규)       |

### 1.6 검증 항목

|   #   | 검증                                       |        기준        |
| :---: | ------------------------------------------ | :----------------: |
|  M1   | 병합 후 엔티티 수 ≤ Step2.1 + Step2.2 합계 |   중복 제거 확인   |
|  M2   | 모든 WorkType에 BELONGS_TO 관계 존재       |        ≥95%        |
|  M3   | HAS_CHILD 계층 일관성                      | 부모 없는 자식 = 0 |
|  M4   | REFERENCES target이 실제 존재              |        100%        |

### 1.7 예상 소요 시간

| 작업            |      시간       |
| --------------- | :-------------: |
| 병합 로직 구현  |      40분       |
| BELONGS_TO 생성 |      20분       |
| HAS_CHILD 생성  |      15분       |
| REFERENCES 생성 |      15분       |
| 검증            |      15분       |
| **합계**        | **~1시간 45분** |

---

## 2. Step 2.4: 엔티티 정규화 & 중복 제거

### 2.1 목적

다른 청크에서 같은 대상을 다른 이름으로 추출한 경우를 통합하고, 오추출/이상치를 필터링한다.

### 2.2 구현 파일

```
phase2_extraction/step4_normalizer.py
```

### 2.3 전체 처리 흐름

```
merged_entities.json
    ↓
[A] 문자열 정규화 (공백, 특수문자, 유니코드)
    ↓
[B] 규칙 기반 중복 제거 (normalized_name 매칭)
    ↓
[C] 유사도 기반 중복 후보 탐지 (편집거리 + Jaccard)
    ↓
[D] 관계 방향 보정 (Equipment→Labor → WorkType→Labor)
    ↓
[E] 이상치 필터링 (수량, confidence)
    ↓
[F] 엔티티 ID 부여 (글로벌 유니크 키)
    ↓
normalized_entities.json
```

### 2.4 세부 작업

#### A. 문자열 정규화

```python
def normalize_entity_name(name: str, entity_type: str) -> str:
    """
    단계별 정규화:
    1. 유니코드 정규화: NFKC (㎥→m³, ㎡→m² 등)
    2. 공백 정규화: 연속 공백 → 단일 공백, 전후 공백 제거
    3. Labor 전용: LABOR_NORMALIZE_MAP 적용 (config.py)
       "보 통 인 부" → "보통인부"
    4. 단위 통일: "m³" ↔ "㎥", "ton" ↔ "t" ↔ "톤"
    5. 괄호 정규화: 전각 괄호 → 반각, 불필요 공백 제거
    """
```

**단위 통일 매핑**:
```python
UNIT_NORMALIZE = {
    "㎥": "m³", "㎡": "m²", "㎝": "cm", "㎜": "mm",
    "㎞": "km", "㏊": "ha", "ℓ": "L",
    "톤": "ton", "t": "ton",
    "키로와트": "kW", "KW": "kW",
    "시간": "hr", "시": "hr",
}
```

#### B. 규칙 기반 중복 제거

```python
def deduplicate_exact(entities: list[Entity]) -> list[Entity]:
    """
    그룹핑 키: (entity_type, normalized_name)
    
    같은 키의 엔티티가 여러 청크에서 추출된 경우:
    1. 대표 엔티티 1개 선정 (가장 높은 confidence)
    2. source_chunk_ids에 모든 출처 청크 기록
    3. quantity/spec이 다른 경우 → 별개 엔티티 유지
       예: "굴착기 0.4m³" vs "굴착기 0.7m³" → 서로 다른 엔티티
    """
```

**그룹핑 키 설계**:
| 엔티티 유형 | 유니크 키                                    | 이유                             |
| ----------- | -------------------------------------------- | -------------------------------- |
| WorkType    | `(type, normalized_name, spec)`              | 같은 공종도 규격별로 다른 투입량 |
| Labor       | `(type, normalized_name)`                    | "보통인부"는 어디서든 동일       |
| Equipment   | `(type, normalized_name, spec)`              | 규격별 다른 장비                 |
| Material    | `(type, normalized_name, spec)`              | 규격별 다른 자재                 |
| Note        | `(type, normalized_name, source_section_id)` | 섹션별 다른 조건                 |
| Standard    | `(type, normalized_name)`                    | 기준은 유일                      |
| Section     | `(type, code)`                               | section_id가 유니크 키           |

#### C. 유사도 기반 중복 후보 탐지

```python
def find_similar_candidates(entities: list[Entity]) -> list[tuple[Entity, Entity, float]]:
    """
    1. 같은 type 내에서만 비교 (다른 type끼리는 비교 안함)
    2. 비교 방법:
       a. Jaccard 유사도 (문자 n-gram, n=2) > 0.7
       b. 편집거리 / max(len1, len2) < 0.3
    3. 임계값 이상인 쌍을 후보로 출력
    4. 수동 확인용 리포트 생성 (자동 병합하지 않음)
    
    예상 후보: ~200~500쌍
    → Step 2.5에서 LLM으로 최종 판단 또는 수동 확인
    """
```

**왜 자동 병합하지 않는가**: 
- "콘크리트공" vs "콘크리트타설" → 유사하지만 완전히 다른 개념 (노무 vs 공종)
- 자동 병합 시 오류 위험 > 수동 확인 비용

#### D. 관계 방향 보정

```python
def fix_relation_directions(relationships: list[Relationship]) -> list[Relationship]:
    """
    심층 검증에서 발견된 774건(3.9%) 관계 방향 오류 수정.
    
    보정 규칙:
    1. Equipment ──REQUIRES_LABOR──→ Labor (159건)
       → source_type을 WorkType으로 변경
       → 해당 Equipment가 속한 WorkType을 찾아서 source를 교체
       → 찾지 못하면 warnings에 기록
       
    2. Equipment ──USES_MATERIAL──→ Material (297건)
       → 동일 로직
       
    3. Equipment ──REQUIRES_EQUIPMENT──→ Equipment (93건)
       → 자기참조: 첫 번째를 WorkType으로 재분류 시도
       → 실패 시 관계 삭제 + warnings
       
    4. WorkType ──HAS_NOTE──→ WorkType (64건)
       → target을 Note 타입으로 재분류
       → target의 name이 실제 주석 내용이면 재분류, 아니면 관계 삭제
       
    5. WorkType ──REQUIRES_EQUIPMENT──→ WorkType (48건)
       → target을 Equipment 타입으로 재분류 시도
    """
```

#### E. 이상치 필터링

```python
def filter_outliers(entities: list[Entity], relationships: list[Relationship]):
    """
    1. 수량 이상치:
       - REQUIRES_LABOR: quantity > 50 → warnings + confidence를 0.3으로 하향
       - REQUIRES_EQUIPMENT: quantity > 20 → 동일
       - 음수 수량 → 제거
       
    2. confidence 이상치:
       - entity.confidence == 0.0 → 제거 (실패 건)
       - entity.confidence < 0.3 → warnings 추가
       
    3. 빈 이름:
       - entity.name == "" 또는 None → 제거
    """
```

#### F. 엔티티 ID 부여

```python
def assign_entity_ids(entities: list[Entity]) -> dict[str, int]:
    """
    글로벌 유니크 ID 부여 (Supabase 저장 시 참조 무결성 위함)
    
    ID 형식: 타입 prefix + 순번
    - WorkType: W-0001 ~ W-NNNN
    - Labor: L-0001 ~ L-NNNN
    - Equipment: E-0001 ~ E-NNNN
    - Material: M-0001 ~ M-NNNN
    - Section: S-0001 ~ S-NNNN
    - Note: N-0001 ~ N-NNNN
    - Standard: ST-0001 ~ ST-NNNN
    
    관계의 source/target도 entity_id로 치환
    """
```

### 2.5 출력 파일

```
phase2_output/normalized_entities.json
```

| 필드                             |               예상 값               |
| -------------------------------- | :---------------------------------: |
| total_entities                   | ~8,000~12,000 (정규화 후 대폭 감소) |
| total_relationships              |           ~28,000~32,000            |
| dedup_stats.merged_pairs         |            ~3,000~5,000             |
| dedup_stats.removed_outliers     |                ~100                 |
| dedup_stats.direction_fixed      |                ~700                 |
| similar_candidates (수동 확인용) |              ~200~500               |

### 2.6 검증 항목

|   #   | 검증                         |               기준               |
| :---: | ---------------------------- | :------------------------------: |
|  N1   | 정규화 전후 엔티티 수 감소율 |        30~60% (적정 범위)        |
|  N2   | 관계 방향 오류               |          0% (전수 보정)          |
|  N3   | 수량 이상치                  |      0건 (필터링/보정 완료)      |
|  N4   | 빈 이름 엔티티               |               0건                |
|  N5   | entity_id 참조 무결성        | 관계의 source/target이 모두 유효 |

### 2.7 예상 소요 시간

| 작업                |      시간       |
| ------------------- | :-------------: |
| 문자열 정규화 구현  |      20분       |
| 규칙 기반 중복 제거 |      30분       |
| 유사도 후보 탐지    |      25분       |
| 관계 방향 보정      |      25분       |
| 이상치 필터링       |      15분       |
| ID 부여             |      10분       |
| 검증                |      15분       |
| **합계**            | **~2시간 20분** |

---

## 3. Step 2.5: 품질 검증

### 3.1 목적

정규화된 최종 데이터가 Supabase 저장 기준을 충족하는지 **6가지 차원**으로 검증한다.

### 3.2 구현 파일

```
phase2_extraction/step5_extraction_validator.py
```

### 3.3 자동 검증 항목 (E1~E4, E6)

|   #    | 검증 항목        |           기준            | 자동화 방법                                       |
| :----: | ---------------- | :-----------------------: | ------------------------------------------------- |
| **E1** | 엔티티 커버리지  |  ≥90% 청크에서 1+ 엔티티  | `sum(1 for ext if len(ext.entities) > 0) / total` |
| **E2** | 관계 무결성      |  source/target 100% 유효  | entity_id 참조 검사                               |
| **E3** | 수량 단위 완전성 | quantity 있으면 unit 필수 | 필드 존재 검사                                    |
| **E4** | 고아 노드 비율   |            ≤5%            | 관계에 참여하지 않는 엔티티 비율                  |
| **E6** | 할루시네이션     |            ≤2%            | 엔티티 이름 원본 검색 (보정된 로직)               |

### 3.4 LLM 샘플 검증 (E5)

```python
async def llm_sample_audit(
    samples: list[ChunkExtraction],
    chunks: dict[str, dict],
    n_samples: int = 50,
) -> AuditResult:
    """
    50개 랜덤 샘플에 대해 LLM이 4가지 항목을 검증:
    
    1. [Completeness] 원본에 있는 엔티티를 빠짐없이 추출했는가?
    2. [Accuracy] 추출된 엔티티의 이름/수량/단위가 정확한가?
    3. [Hallucination] 원본에 없는 엔티티를 생성하지 않았는가?
    4. [Relationship] 관계가 올바르고 빠짐없는가?
    
    각 항목 0~1 점수, 평균 ≥ 0.9 기준
    """
```

**프롬프트 설계**:
```
당신은 건설공사 표준품셈 데이터 품질 감사원입니다.

## 원본 텍스트
{chunk_text}

## 추출 결과
{extraction_json}

## 감사 항목
아래 4가지를 0.0~1.0 점수로 평가하세요:
1. Completeness: 원본의 모든 엔티티를 빠짐없이 추출했는가?
2. Accuracy: 이름/수량/단위가 정확한가?
3. Hallucination: 원본에 없는 정보를 생성하지 않았는가?
4. Relationship: 관계가 올바르고 빠짐없는가?

각 항목에 대해 점수와 근거를 제공하세요.
```

### 3.5 PASS / FAIL 판정

```python
PASS_CRITERIA = {
    "E1_coverage": 0.90,
    "E2_integrity": 1.00,
    "E3_unit_completeness": 0.95,
    "E4_orphan_rate": 0.05,     # ≤ 5%
    "E5_sample_accuracy": 0.90,
    "E6_hallucination": 0.02,   # ≤ 2%
}

def judge(results: dict) -> str:
    """
    PASS: 모든 항목 기준 충족
    CONDITIONAL_PASS: E5만 미달 (나머지 충족)
    FAIL: E1, E2, E4, E6 중 하나라도 미달
    """
```

### 3.6 출력 파일

```
phase2_output/extraction_report.json
```

```json
{
  "timestamp": "2026-02-11T...",
  "verdict": "PASS",
  "scores": {
    "E1_coverage": 0.999,
    "E2_integrity": 1.000,
    "E3_unit_completeness": 0.97,
    "E4_orphan_rate": 0.03,
    "E5_sample_accuracy": 0.94,
    "E6_hallucination": 0.012
  },
  "details": { ... },
  "recommendations": [ ... ]
}
```

### 3.7 FAIL 시 대응

```
FAIL 판정 → 원인 분석 → Step 2.2 또는 2.4 재실행
  - E1 실패 → 미커버 청크 분석 → 추가 LLM 추출
  - E2 실패 → 관계의 참조 오류 수정
  - E4 실패 → 고아 엔티티 관계 보충 또는 제거
  - E6 실패 → 할루시네이션 엔티티 제거
```

### 3.8 예상 소요 시간

| 작업                     |      시간       |
| ------------------------ | :-------------: |
| E1~E4, E6 자동 검증 구현 |      30분       |
| E5 LLM 샘플 감사 구현    |      30분       |
| E5 실행 (50건 × Gemini)  |      15분       |
| 판정 로직 + 리포트 생성  |      15분       |
| **합계**                 | **~1시간 30분** |

---

## 4. Step 2.6: Supabase 저장 & 임베딩

### 4.1 목적

검증 완료된 엔티티/관계를 Supabase PostgreSQL에 저장하고, 벡터 검색을 위한 임베딩을 생성한다.

### 4.2 구현 파일

```
phase2_extraction/step6_supabase_loader.py
```

### 4.3 DB 스키마 (DDL)

```sql
-- ═══════════════════════════════════════════════════════════
-- 1. 엔티티 테이블
-- ═══════════════════════════════════════════════════════════
CREATE TABLE graph_entities (
    id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
    entity_id TEXT UNIQUE NOT NULL,           -- "W-0001", "L-0001" 등
    type TEXT NOT NULL,                        -- WorkType, Labor, Equipment, ...
    name TEXT NOT NULL,
    normalized_name TEXT NOT NULL,
    code TEXT,                                 -- 품셈 코드 (있는 경우)
    spec TEXT,                                 -- 규격/사양
    unit TEXT,                                 -- 단위
    quantity FLOAT,                            -- 대표 수량
    properties JSONB DEFAULT '{}',
    confidence FLOAT DEFAULT 1.0,
    source_chunk_ids TEXT[] DEFAULT '{}',       -- 추출 원본 청크 목록 (배열)
    source_section_id TEXT,
    department TEXT,
    chapter TEXT,
    source_method TEXT,                        -- table_rule, llm, merged
    embedding VECTOR(768),                     -- text-embedding-004
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ═══════════════════════════════════════════════════════════
-- 2. 관계 테이블
-- ═══════════════════════════════════════════════════════════
CREATE TABLE graph_relationships (
    id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
    source_entity_id TEXT NOT NULL REFERENCES graph_entities(entity_id),
    target_entity_id TEXT NOT NULL REFERENCES graph_entities(entity_id),
    type TEXT NOT NULL,                        -- REQUIRES_LABOR, BELONGS_TO, ...
    quantity FLOAT,                            -- 투입 수량
    unit TEXT,                                 -- 투입 단위
    per_unit TEXT,                             -- 기준 단위 (1m³당 등)
    properties JSONB DEFAULT '{}',
    source_chunk_id TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ═══════════════════════════════════════════════════════════
-- 3. 청크 테이블 (기존 unit_costs 확장)
-- ═══════════════════════════════════════════════════════════
CREATE TABLE graph_chunks (
    id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
    chunk_id TEXT UNIQUE NOT NULL,
    section_id TEXT NOT NULL,
    department TEXT,
    chapter TEXT,
    title TEXT,
    content TEXT,                               -- text + tables 결합
    metadata JSONB DEFAULT '{}',                -- 원본 메타데이터
    entity_count INT DEFAULT 0,
    relationship_count INT DEFAULT 0,
    embedding VECTOR(768),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ═══════════════════════════════════════════════════════════
-- 4. 인덱스
-- ═══════════════════════════════════════════════════════════
-- 엔티티
CREATE INDEX idx_ge_type ON graph_entities(type);
CREATE INDEX idx_ge_dept ON graph_entities(department);
CREATE INDEX idx_ge_section ON graph_entities(source_section_id);
CREATE INDEX idx_ge_norm_name ON graph_entities(normalized_name);
CREATE INDEX idx_ge_name_fts ON graph_entities 
    USING GIN (to_tsvector('simple', name));
CREATE INDEX idx_ge_embedding ON graph_entities 
    USING hnsw (embedding vector_cosine_ops)
    WITH (m = 16, ef_construction = 64);

-- 관계
CREATE INDEX idx_gr_type ON graph_relationships(type);
CREATE INDEX idx_gr_source ON graph_relationships(source_entity_id);
CREATE INDEX idx_gr_target ON graph_relationships(target_entity_id);

-- 청크
CREATE INDEX idx_gc_section ON graph_chunks(section_id);
CREATE INDEX idx_gc_dept ON graph_chunks(department);
CREATE INDEX idx_gc_embedding ON graph_chunks 
    USING hnsw (embedding vector_cosine_ops)
    WITH (m = 16, ef_construction = 64);

-- ═══════════════════════════════════════════════════════════
-- 5. RLS 정책 (공개 읽기)
-- ═══════════════════════════════════════════════════════════
ALTER TABLE graph_entities ENABLE ROW LEVEL SECURITY;
ALTER TABLE graph_relationships ENABLE ROW LEVEL SECURITY;
ALTER TABLE graph_chunks ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Public read" ON graph_entities FOR SELECT USING (true);
CREATE POLICY "Public read" ON graph_relationships FOR SELECT USING (true);
CREATE POLICY "Public read" ON graph_chunks FOR SELECT USING (true);
```

### 4.4 데이터 적재 프로세스

```python
async def load_to_supabase(
    normalized_data: dict,
    chunks: list[dict],
    batch_size: int = 500,
) -> LoadResult:
    """
    처리 순서 (참조 무결성):
    
    Phase A: 엔티티 적재
    1. graph_entities에 엔티티 INSERT (batch 500건씩)
    2. entity_id → Supabase id 매핑 테이블 구축
    
    Phase B: 관계 적재
    3. source_entity_id, target_entity_id를 entity_id로 치환
    4. graph_relationships에 INSERT (batch 500건씩)
    
    Phase C: 청크 적재
    5. graph_chunks에 청크 데이터 INSERT
    6. entity_count, relationship_count 집계
    
    Phase D: 임베딩 생성
    7. 엔티티 임베딩: name + spec + type → text-embedding-004
    8. 청크 임베딩: content → text-embedding-004
    9. UPDATE로 embedding 컬럼 채우기
    """
```

### 4.5 임베딩 생성

```python
def build_embedding_text(entity: dict) -> str:
    """
    엔티티를 임베딩용 텍스트로 변환.
    
    WorkType: "{name} {spec} ({department} {chapter})"
    Labor:    "{name} 노무 인력"
    Equipment: "{name} {spec} 장비"
    Material:  "{name} {spec} 자재"
    Section:   "{section_id} {title} ({department})"
    Note:      "{name} 주석 조건"
    Standard:  "{name} {code} 기준 규격"
    """
```

| 임베딩 대상      | 모델               | 차원  | 예상 토큰 |  예상 비용  |
| ---------------- | ------------------ | :---: | :-------: | :---------: |
| 엔티티 ~10,000개 | text-embedding-004 |  768  |   ~500K   |   ~$0.003   |
| 청크 ~2,100개    | text-embedding-004 |  768  |   ~1.6M   |   ~$0.010   |
| **합계**         |                    |       |   ~2.1M   | **~$0.013** |

### 4.6 검증 항목

|   #   | 검증             | 방법                                                                                                         |
| :---: | ---------------- | ------------------------------------------------------------------------------------------------------------ |
|  S1   | 적재 건수 일치   | `SELECT COUNT(*) FROM graph_entities` == normalized count                                                    |
|  S2   | 참조 무결성      | `SELECT * FROM graph_relationships WHERE source_entity_id NOT IN (SELECT entity_id FROM graph_entities)` = 0 |
|  S3   | 임베딩 NULL 없음 | `SELECT COUNT(*) FROM graph_entities WHERE embedding IS NULL` = 0                                            |
|  S4   | 검색 동작 확인   | 샘플 쿼리 3개 실행하여 결과 확인                                                                             |

### 4.7 샘플 검색 쿼리

```sql
-- 1. "콘크리트 타설"과 유사한 엔티티 찾기
SELECT entity_id, type, name, spec,
       1 - (embedding <=> query_embedding) AS similarity
FROM graph_entities
ORDER BY embedding <=> query_embedding
LIMIT 10;

-- 2. 특정 공종의 필요 인력 조회 (그래프 탐색)
SELECT e2.name AS labor, r.quantity, r.unit
FROM graph_entities e1
JOIN graph_relationships r ON r.source_entity_id = e1.entity_id
JOIN graph_entities e2 ON r.target_entity_id = e2.entity_id
WHERE e1.name LIKE '%콘크리트 타설%'
  AND r.type = 'REQUIRES_LABOR';

-- 3. 하이브리드 검색 (벡터 + 그래프)
-- Phase 3에서 함수화
```

### 4.8 예상 소요 시간

| 작업               |    시간    |
| ------------------ | :--------: |
| DDL 마이그레이션   |    15분    |
| 엔티티 적재 로직   |    25분    |
| 관계 적재 로직     |    20분    |
| 청크 적재 로직     |    15분    |
| 임베딩 생성 & 적재 |    30분    |
| 검증 & 샘플 쿼리   |    15분    |
| **합계**           | **~2시간** |

---

## 5. 전체 실행 계획

### 5.1 실행 순서

```
Step 2.3  ──→  Step 2.4  ──→  Step 2.5  ──→  Step 2.6
(병합)        (정규화)       (검증)          (저장)
 ~1h45m        ~2h20m         ~1h30m          ~2h
```

### 5.2 일정 (예상)

|     단계     | 작업                         |    예상 시간    |  누적  |
| :----------: | ---------------------------- | :-------------: | :----: |
| **Step 2.3** | 병합 + 구조적 관계 생성      |   1시간 45분    | 1h 45m |
| **Step 2.4** | 정규화 + 중복 제거 + 보정    |   2시간 20분    | 4h 05m |
| **Step 2.5** | 자동 검증 + LLM 샘플 감사    |   1시간 30분    | 5h 35m |
| **Step 2.6** | Supabase DDL + 적재 + 임베딩 |      2시간      | 7h 35m |
|   **합계**   |                              | **~7시간 35분** |        |

### 5.3 비용 추정

| 항목                     | 모델               |    비용    |
| ------------------------ | ------------------ | :--------: |
| Step 2.5 LLM 감사 (50건) | Gemini 3.0 Flash   |   ~$0.02   |
| Step 2.6 임베딩          | text-embedding-004 |  ~$0.013   |
| **총 추가 비용**         |                    | **~$0.03** |

### 5.4 최종 산출물

```
phase2_output/
├── table_entities.json         ← Step 2.1 (완료)
├── llm_entities.json           ← Step 2.2 (완료)
├── merged_entities.json        ← Step 2.3 (다음)
├── normalized_entities.json    ← Step 2.4
├── extraction_report.json      ← Step 2.5
├── similar_candidates.json     ← Step 2.4 (수동 확인용)
├── quality_report.txt          ← (완료)
└── quality_report_deep.txt     ← (완료)

Supabase:
├── graph_entities              ← ~10,000 rows + 768-dim embeddings
├── graph_relationships         ← ~30,000 rows
└── graph_chunks                ← ~2,100 rows + 768-dim embeddings
```

---

## 6. 리스크 & 대응

|   #   | 리스크                                               | 가능성 | 영향  | 대응                                                               |
| :---: | ---------------------------------------------------- | :----: | :---: | ------------------------------------------------------------------ |
|   1   | 중복 제거 과잉 (서로 다른 엔티티를 같은 것으로 합침) |  중간  | 높음  | 자동 병합은 `normalized_name` 정확 매칭만. 유사도 후보는 수동 확인 |
|   2   | 관계 방향 보정 실패 (원본 WorkType 찾기 불가)        |  낮음  | 중간  | 실패 건은 관계 삭제 + warnings 기록                                |
|   3   | Supabase Rate Limit (임베딩 생성 시)                 |  낮음  | 낮음  | batch_size=100 + 1초 딜레이                                        |
|   4   | text-embedding-004 한국어 품질                       |  중간  | 중간  | 임베딩 텍스트에 한국어 context 풍부하게 포함                       |
|   5   | E5 LLM 감사 비용 추가                                |  낮음  | 낮음  | 50건 × Gemini Flash ≈ $0.02                                        |

---

## 7. Phase 3 연결 미리보기

Step 2.6 완료 후 Phase 3에서 구현할 검색 함수:

```sql
-- match_entities(): 벡터 유사도 검색
-- get_entity_neighbors(): N-hop 그래프 탐색
-- hybrid_graph_search(): 벡터 + 그래프 + 키워드 결합
```

```
사용자 질의: "콘크리트 타설 1m³에 필요한 인력은?"
    ↓
1. match_entities("콘크리트 타설") → W-0312
2. get_entity_neighbors(W-0312, type="REQUIRES_LABOR")
   → [L-0005 보통인부 0.67인, L-0023 콘크리트공 0.33인]
3. 응답 생성 with source context
```
