# -*- coding: utf-8 -*-
"""Step 2.2: LLM ê¸°ë°˜ ì—”í‹°í‹° & ê´€ê³„ ì¶”ì¶œ

Step 2.1(í…Œì´ë¸” ê·œì¹™ ì¶”ì¶œ)ì—ì„œ ì»¤ë²„í•˜ì§€ ëª»í•œ ì²­í¬ë¥¼ Gemini 3.0 Flashë¡œ ì²˜ë¦¬í•œë‹¤.

ëŒ€ìƒ:
  1. í…Œì´ë¸”ì´ ì—†ëŠ” í…ìŠ¤íŠ¸ ì „ìš© ì²­í¬ (364ê±´)
  2. D_ê¸°íƒ€ / C_êµ¬ë¶„ì„¤ëª… í…Œì´ë¸”ì„ ê°€ì§„ ì²­í¬ (~2,092ê±´)
  3. Step 2.1ì—ì„œ WorkTypeì´ ì¶”ì¶œë˜ì§€ ì•Šì€ ì²­í¬
  4. Step 2.1 ê²½ê³ ê°€ ìˆëŠ” ì²­í¬ (ì¸ì‹ ë¶ˆê°€ í—¤ë” ë“±)

llm-structured-extraction ìŠ¤í‚¬ ì ìš©:
  - Pydantic ìŠ¤í‚¤ë§ˆë¡œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ê°•ì œ
  - Few-shot + Chain-of-Thought í”„ë¡¬í”„íŠ¸
  - ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ (ë™ì‹œ 5ê°œ)
  - ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
  - ìê¸° êµì • (Self-Correction) íŒ¨í„´
"""
import asyncio
import json
import os
import re
import sys
import time
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Optional

from dotenv import load_dotenv
from openai import OpenAI
from pydantic import BaseModel, Field

from config import (
    CHUNKS_FILE, PHASE2_OUTPUT, TABLE_ENTITIES_FILE, LLM_ENTITIES_FILE,
    LLM_MODEL, LLM_TEMPERATURE, LLM_CONCURRENCY, LLM_RETRY_COUNT, LLM_MAX_TOKENS,
)
from schemas import (
    Entity, Relationship, ChunkExtraction, BatchResult,
    EntityType, RelationType,
)

ISOLATED_CHUNKS = {
    "C-0172", "C-0578-A", "C-0578-B", 
    "C-0623-A", "C-0623-B", "C-0759", 
    "C-0923", "C-1124", "C-1149"
}

sys.stdout.reconfigure(encoding="utf-8")


# â”€â”€â”€ .env ë¡œë“œ & DeepSeek í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv(Path(__file__).parent.parent / ".env")

client = OpenAI(
    api_key=os.environ["DEEPSEEK_API_KEY"],
    base_url="https://api.deepseek.com",
)


# â”€â”€â”€ LLM ì¶œë ¥ìš© Pydantic ìŠ¤í‚¤ë§ˆ (ê°„ì†Œí™”) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Why: DeepSeek JSON modeëŠ” í”„ë¡¬í”„íŠ¸ì— ìŠ¤í‚¤ë§ˆë¥¼ í¬í•¨í•´ì•¼ í•¨.
#      Phase 2ì˜ Entity ì „ì²´ ìŠ¤í‚¤ë§ˆ ëŒ€ì‹  ì¶”ì¶œì— í•„ìˆ˜ì¸ í•„ë“œë§Œ í¬í•¨.

class LLMEntity(BaseModel):
    """LLMì´ ì¶”ì¶œí•  ì—”í‹°í‹°"""
    type: str = Field(description="ì—”í‹°í‹° ìœ í˜•: WorkType, Labor, Equipment, Material, Note, Standard ì¤‘ í•˜ë‚˜")
    name: str = Field(description="ì—”í‹°í‹° ì´ë¦„ (ì›ë³¸ í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •í™•í•œ ì´ë¦„)")
    spec: Optional[str] = Field(None, description="ê·œê²©/ì‚¬ì–‘ (ì˜ˆ: 0.6mÂ³, D13, 25-24-15)")
    unit: Optional[str] = Field(None, description="ë‹¨ìœ„ (ì˜ˆ: ì¸, mÂ³, ëŒ€, ton)")
    quantity: Optional[float] = Field(None, description="ìˆ˜ëŸ‰ (ìˆ«ìë§Œ)")


class LLMRelationship(BaseModel):
    """LLMì´ ì¶”ì¶œí•  ê´€ê³„"""
    source: str = Field(description="ì¶œë°œ ì—”í‹°í‹° ì´ë¦„")
    target: str = Field(description="ë„ì°© ì—”í‹°í‹° ì´ë¦„")
    relation_type: str = Field(description="ê´€ê³„: REQUIRES_LABOR, REQUIRES_EQUIPMENT, USES_MATERIAL, HAS_NOTE, APPLIES_STANDARD ì¤‘ í•˜ë‚˜")
    relation_type: str = Field(description="ê´€ê³„: REQUIRES_LABOR, REQUIRES_EQUIPMENT, USES_MATERIAL, HAS_NOTE, APPLIES_STANDARD ì¤‘ í•˜ë‚˜")
    quantity: Optional[float] = Field(None, description="íˆ¬ì… ìˆ˜ëŸ‰")
    unit: Optional[str] = Field(None, description="íˆ¬ì… ë‹¨ìœ„")
    per_unit: Optional[str] = Field(None, description="ê¸°ì¤€ ë‹¨ìœ„ (ì˜ˆ: '1m3ë‹¹', '100më‹¹')")
    # ğŸ’¡ [Track A] ê·œê²©ë³„ ìˆ˜ëŸ‰ ì¶”ì ì„ ìœ„í•œ ììœ í˜• Dict
    # Why: ë§¤íŠ¸ë¦­ìŠ¤(2D) í‘œì—ì„œ ë™ì¼ source-target ìŒì´ ê·œê²©ë³„ë¡œ ë‹¤ë¥¸ ìˆ˜ëŸ‰ì„ ê°€ì§ˆ ë•Œ
    #       {"source_spec": "200mm"} í˜•íƒœë¡œ ê·œê²©ì„ ê¸°ë¡í•˜ì—¬ ê´€ê³„ë¥¼ ê³ ìœ í•˜ê²Œ ì‹ë³„
    properties: Optional[dict] = Field(default_factory=dict, description="ì¶”ê°€ ì†ì„± (source_spec ë“±)")


class LLMExtractionResult(BaseModel):
    """LLM ì¶”ì¶œ ì „ì²´ ê²°ê³¼"""
    # ğŸ’¡ [Track A] Chain-of-Thought ë²„í¼
    # Why: ë§¤íŠ¸ë¦­ìŠ¤ í‘œ íŒŒì‹± ì‹œ LLMì´ "ëª‡ ê°œ ê·œê²©ì„ ì „ê°œí•  ê²ƒì¸ì§€" ì‚¬ê³  ê³¼ì •ì„ ê¸°ë¡
    #       ì´ë¥¼ í†µí•´ ëˆ„ë½ ì—¬ë¶€ë¥¼ ì‚¬í›„ ê²€ì¦í•  ìˆ˜ ìˆìŒ (ë””ë²„ê¹…ìš©, íŒŒì´í”„ë¼ì¸ì— ì˜í–¥ ì—†ìŒ)
    matrix_analysis_scratchpad: Optional[str] = Field(
        default="",
        description="ë‹¤ì¤‘ ê·œê²© í‘œ íŒŒì‹± ì‹œ LLMì˜ ì‚¬ê³  ê³¼ì • ê¸°ë¡"
    )
    entities: list[LLMEntity] = Field(default_factory=list)
    relationships: list[LLMRelationship] = Field(default_factory=list)
    summary: str = Field(default="", description="ì²­í¬ ë‚´ìš© 1ì¤„ ìš”ì•½ (í•œêµ­ì–´)")
    confidence: float = Field(default=0.8, ge=0, le=1, description="ì¶”ì¶œ ì‹ ë¢°ë„ 0~1")


# â”€â”€â”€ í”„ë¡¬í”„íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ê±´ì„¤ í‘œì¤€í’ˆì…ˆ ë¬¸ì„œì—ì„œ ì—”í‹°í‹°(ê°œì²´)ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì—”í‹°í‹° ìœ í˜•
- **WorkType**: ê³µì¢…/ì‘ì—… (ì˜ˆ: ì½˜í¬ë¦¬íŠ¸ íƒ€ì„¤, ì² ê·¼ ê°€ê³µ, ê±°í‘¸ì§‘ ì„¤ì¹˜)
- **Labor**: ë…¸ë¬´/ì¸ë ¥ (ì˜ˆ: íŠ¹ë³„ì¸ë¶€, ë³´í†µì¸ë¶€, ì² ê·¼ê³µ, ë¹„ê³„ê³µ, í˜•í‹€ëª©ê³µ)
- **Equipment**: ì¥ë¹„/ê¸°ê³„ (ì˜ˆ: êµ´ì°©ê¸°, í¬ë ˆì¸, ë ˆë¯¸ì½˜, íŒí”„ì¹´)
- **Material**: ìì¬/ì¬ë£Œ (ì˜ˆ: ì‹œë©˜íŠ¸, ê³¨ì¬, ì² ê·¼, ê±°í‘¸ì§‘íŒ)
- **Note**: ì£¼ì„/ì¡°ê±´/í• ì¦ (ì˜ˆ: í• ì¦ë¥ , ì ìš© ì¡°ê±´, ë³´ì •ê³„ìˆ˜)
- **Standard**: ì ìš© ê¸°ì¤€/ê·œê²© (ì˜ˆ: KCS, KDS, ì½˜í¬ë¦¬íŠ¸ í‘œì¤€ì‹œë°©ì„œ)

## ê´€ê³„ ìœ í˜•
- **REQUIRES_LABOR**: ê³µì¢… â†’ ë…¸ë¬´ (ì¸ë ¥ íˆ¬ì…, ë°˜ë“œì‹œ quantity/unit í¬í•¨)
- **REQUIRES_EQUIPMENT**: ê³µì¢… â†’ ì¥ë¹„ (ì¥ë¹„ íˆ¬ì…)
- **USES_MATERIAL**: ê³µì¢… â†’ ìì¬ (ìì¬ ì‚¬ìš©)
- **HAS_NOTE**: ê³µì¢…/ì„¹ì…˜ â†’ ì£¼ì„ (ì¡°ê±´/í• ì¦)
- **APPLIES_STANDARD**: ê³µì¢… â†’ ê¸°ì¤€ (ì ìš© ê·œê²©)

## ê·œì¹™
1. ì›ë³¸ í…ìŠ¤íŠ¸ì— **ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”** ì´ë¦„ê³¼ ìˆ˜ì¹˜ë§Œ ì¶”ì¶œí•œë‹¤ (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
2. ìˆ˜ëŸ‰ì€ ë°˜ë“œì‹œ ì›ë³¸ì˜ ìˆ«ìë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤
3. ê°™ì€ ì—”í‹°í‹°ë¥¼ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ ì¤‘ë³µ ì¶”ì¶œí•˜ì§€ ì•ŠëŠ”ë‹¤
4. í…Œì´ë¸”ì´ ìˆìœ¼ë©´ í–‰/ì—´ êµ¬ì¡°ë¥¼ ì •í™•íˆ í•´ì„í•œë‹¤
5. '1mÂ³ë‹¹', '100më‹¹' ë“± ê¸°ì¤€ ë‹¨ìœ„(per_unit)ë„ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ ì¶”ì¶œí•œë‹¤.
6. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” confidenceë¥¼ ë‚®ê²Œ ì„¤ì •í•œë‹¤
7. ğŸš¨ **[ë§¤íŠ¸ë¦­ìŠ¤ í‘œ ì „ê°œ ê·œì¹™]** ê°€ë¡œì¶•ì— ì—¬ëŸ¬ ê·œê²©(63mm, 75mm, 200mm ë“±)ì´ ë‚˜ì—´ëœ í‘œëŠ”
   ì ˆëŒ€ ì¤‘ê°„ ê·œê²©ì„ ìƒëµí•˜ê±°ë‚˜ "ë“±"ìœ¼ë¡œ ë¬¶ì§€ ë§ˆì‹­ì‹œì˜¤.
   **ëª¨ë“  ê·œê²©ì— ëŒ€í•´ ë…ë¦½ëœ ê´€ê³„(relationship) ê°ì²´ë¥¼ 100% ì „ê°œ(Unroll)**í•´ì•¼ í•©ë‹ˆë‹¤.
8. ê° ê´€ê³„ì˜ `properties.source_spec`ì— í•´ë‹¹ ìˆ˜ëŸ‰ì˜ **ì •í™•í•œ ê·œê²© ë¬¸ìì—´**ì„ ë°˜ë“œì‹œ ê¸°ë¡í•˜ì‹­ì‹œì˜¤.
9. ë§¤íŠ¸ë¦­ìŠ¤ í‘œê°€ ê°ì§€ë˜ë©´ `matrix_analysis_scratchpad`ì— "[ê·œê²© ìˆ˜] Ã— [ì§ì¢… ìˆ˜] = [ì´ ê´€ê³„ ìˆ˜]"
   í˜•íƒœë¡œ ì‚¬ê³  ê³¼ì •ì„ ê¸°ë¡í•œ ë’¤ ì „ê°œë¥¼ ì‹œì‘í•˜ì‹­ì‹œì˜¤.

## ì¶œë ¥ JSON ìŠ¤í‚¤ë§ˆ (ë°˜ë“œì‹œ ì´ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥)
```json
{
  "matrix_analysis_scratchpad": "ë‹¤ì¤‘ ê·œê²© í‘œê°€ ìˆìœ¼ë©´ ì‚¬ê³  ê³¼ì •ì„ ì—¬ê¸°ì— ê¸°ë¡",
  "entities": [{"type": "WorkType|Labor|Equipment|Material|Note|Standard", "name": "ë¬¸ìì—´", "spec": "ë¬¸ìì—´ or null", "unit": "ë¬¸ìì—´ or null", "quantity": ìˆ«ì or null}],
  "relationships": [{
    "source": "ì¶œë°œì—”í‹°í‹°ëª…",
    "target": "ë„ì°©ì—”í‹°í‹°ëª…",
    "relation_type": "REQUIRES_LABOR|REQUIRES_EQUIPMENT|USES_MATERIAL|HAS_NOTE|APPLIES_STANDARD",
    "quantity": ìˆ«ì or null,
    "unit": "ë¬¸ìì—´ or null",
    "per_unit": "ë¬¸ìì—´ or null",
    "properties": {"source_spec": "í•´ë‹¹ ìˆ˜ëŸ‰ì˜ ê·œê²© (ì˜ˆ: 200mm)"}
  }],
  "summary": "1ì¤„ ìš”ì•½ (í•œêµ­ì–´)",
  "confidence": 0.0~1.0
}
```"""


FEW_SHOT_EXAMPLE = """
## ì˜ˆì‹œ 1: ë‹¨ì¼ ê·œê²© (ê¸°ì¡´)

### ì…ë ¥
ì„¹ì…˜: ì½˜í¬ë¦¬íŠ¸ íƒ€ì„¤ (ë ˆë¯¸ì½˜ 25-24-15)
í…ìŠ¤íŠ¸: "1mÂ³ë‹¹ íŠ¹ë³„ì¸ë¶€ 0.33ì¸, ë³´í†µì¸ë¶€ 0.67ì¸, ì½˜í¬ë¦¬íŠ¸ê³µ 0.15ì¸"

### ì¶œë ¥
{
  "matrix_analysis_scratchpad": "",
  "entities": [
    {"type": "WorkType", "name": "ì½˜í¬ë¦¬íŠ¸ íƒ€ì„¤", "spec": "ë ˆë¯¸ì½˜ 25-24-15", "unit": "mÂ³", "quantity": null},
    {"type": "Labor", "name": "íŠ¹ë³„ì¸ë¶€", "spec": null, "unit": "ì¸", "quantity": 0.33},
    {"type": "Labor", "name": "ë³´í†µì¸ë¶€", "spec": null, "unit": "ì¸", "quantity": 0.67},
    {"type": "Labor", "name": "ì½˜í¬ë¦¬íŠ¸ê³µ", "spec": null, "unit": "ì¸", "quantity": 0.15}
  ],
  "relationships": [
    {"source": "ì½˜í¬ë¦¬íŠ¸ íƒ€ì„¤", "target": "íŠ¹ë³„ì¸ë¶€", "relation_type": "REQUIRES_LABOR", "quantity": 0.33, "unit": "ì¸", "per_unit": "1mÂ³ë‹¹", "properties": {"source_spec": "ë ˆë¯¸ì½˜ 25-24-15"}},
    {"source": "ì½˜í¬ë¦¬íŠ¸ íƒ€ì„¤", "target": "ë³´í†µì¸ë¶€", "relation_type": "REQUIRES_LABOR", "quantity": 0.67, "unit": "ì¸", "per_unit": "1mÂ³ë‹¹", "properties": {"source_spec": "ë ˆë¯¸ì½˜ 25-24-15"}},
    {"source": "ì½˜í¬ë¦¬íŠ¸ íƒ€ì„¤", "target": "ì½˜í¬ë¦¬íŠ¸ê³µ", "relation_type": "REQUIRES_LABOR", "quantity": 0.15, "unit": "ì¸", "per_unit": "1mÂ³ë‹¹", "properties": {"source_spec": "ë ˆë¯¸ì½˜ 25-24-15"}}
  ],
  "summary": "ì½˜í¬ë¦¬íŠ¸ íƒ€ì„¤(ë ˆë¯¸ì½˜ 25-24-15) 1mÂ³ë‹¹ ì¸ë ¥íˆ¬ì… ê¸°ì¤€",
  "confidence": 0.95
}

## ì˜ˆì‹œ 2: ë§¤íŠ¸ë¦­ìŠ¤ í‘œ ì „ê°œ (ğŸš¨ í•µì‹¬)

### ì…ë ¥
ì„¹ì…˜: ê°€ìŠ¤ìš© í´ë¦¬ì—í‹¸ë Œ(PE)ê´€ ì ‘í•© ë° ë¶€ì„¤

| êµ¬ë¶„ | 63mm | 200mm |
| --- | --- | --- |
| ë°°ê´€ê³µ | 0.184 | 0.521 |
| íŠ¹ë³„ì¸ë¶€ | 0.052 | 0.113 |

### ì¶œë ¥
{
  "matrix_analysis_scratchpad": "2ê°œ ê·œê²©(63mm, 200mm) Ã— 2ê°œ ì§ì¢…(ë°°ê´€ê³µ, íŠ¹ë³„ì¸ë¶€) = 4ê°œ ê´€ê³„. ëª¨ë‘ ì „ê°œ.",
  "entities": [
    {"type": "WorkType", "name": "ê°€ìŠ¤ìš© í´ë¦¬ì—í‹¸ë Œ(PE)ê´€ ì ‘í•© ë° ë¶€ì„¤", "spec": null, "unit": null, "quantity": null},
    {"type": "Labor", "name": "ë°°ê´€ê³µ", "spec": null, "unit": "ì¸", "quantity": null},
    {"type": "Labor", "name": "íŠ¹ë³„ì¸ë¶€", "spec": null, "unit": "ì¸", "quantity": null}
  ],
  "relationships": [
    {"source": "ê°€ìŠ¤ìš© í´ë¦¬ì—í‹¸ë Œ(PE)ê´€ ì ‘í•© ë° ë¶€ì„¤", "target": "ë°°ê´€ê³µ", "relation_type": "REQUIRES_LABOR", "quantity": 0.184, "unit": "ì¸", "properties": {"source_spec": "63mm"}},
    {"source": "ê°€ìŠ¤ìš© í´ë¦¬ì—í‹¸ë Œ(PE)ê´€ ì ‘í•© ë° ë¶€ì„¤", "target": "ë°°ê´€ê³µ", "relation_type": "REQUIRES_LABOR", "quantity": 0.521, "unit": "ì¸", "properties": {"source_spec": "200mm"}},
    {"source": "ê°€ìŠ¤ìš© í´ë¦¬ì—í‹¸ë Œ(PE)ê´€ ì ‘í•© ë° ë¶€ì„¤", "target": "íŠ¹ë³„ì¸ë¶€", "relation_type": "REQUIRES_LABOR", "quantity": 0.052, "unit": "ì¸", "properties": {"source_spec": "63mm"}},
    {"source": "ê°€ìŠ¤ìš© í´ë¦¬ì—í‹¸ë Œ(PE)ê´€ ì ‘í•© ë° ë¶€ì„¤", "target": "íŠ¹ë³„ì¸ë¶€", "relation_type": "REQUIRES_LABOR", "quantity": 0.113, "unit": "ì¸", "properties": {"source_spec": "200mm"}}
  ],
  "summary": "ê°€ìŠ¤ìš© PEê´€ ì ‘í•© ê·œê²©ë³„(63mm, 200mm) ì¸ë ¥íˆ¬ì… ê¸°ì¤€ â€” ì „ì²´ ì „ê°œ",
  "confidence": 0.95
}
"""


def build_user_prompt(chunk: dict, all_chunks: list[dict] = []) -> str:
    """ì²­í¬ ë°ì´í„° â†’ LLM ì…ë ¥ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    parts = []

    # ì„¹ì…˜ ë©”íƒ€ë°ì´í„°
    parts.append(f"## ì„¹ì…˜ ì •ë³´")
    parts.append(f"- ì„¹ì…˜ID: {chunk.get('section_id', '')}")
    parts.append(f"- ì œëª©: {chunk.get('title', '')}")
    parts.append(f"- ë¶€ë¬¸: {chunk.get('department', '')}")
    parts.append(f"- ì¥: {chunk.get('chapter', '')}")
    if chunk.get('unit_basis'):
        parts.append(f"- ê¸°ì¤€ë‹¨ìœ„: {chunk['unit_basis']}")

    # ë³¸ë¬¸ í…ìŠ¤íŠ¸ (ë¹ˆ í…ìŠ¤íŠ¸ë©´ í˜•ì œ ì°¾ì•„ì„œ ì£¼ì…)
    text = chunk.get("text", "").strip()
    if text:
        parts.append(f"\n## ë³¸ë¬¸ í…ìŠ¤íŠ¸\n{text}")
    else:
        # ë¹ˆ í…ìŠ¤íŠ¸ì¼ ë•Œ í˜•ì œ ì²­í¬ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ë³µì›
        chunk_id = chunk.get("chunk_id", "")
        match = re.match(r"(C-\d+)", chunk_id)
        if match and all_chunks:
            base_id = match.group(1)
            siblings = [c for c in all_chunks 
                        if c.get("chunk_id", "").startswith(base_id) and c.get("text", "").strip()]
            if siblings:
                sibling_text = siblings[0].get("text", "").strip()
                sibling_id = siblings[0].get("chunk_id", "")
                parts.append(f"\n## ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ (ë™ì¼ ì„¹ì…˜ {sibling_id}ì—ì„œ ì°¸ì¡°)")
                parts.append(sibling_text)
                parts.append(f"\nâš ï¸ ìœ„ í…ìŠ¤íŠ¸ëŠ” ë™ì¼ ì„¹ì…˜ì˜ ë‹¤ë¥¸ ì²­í¬ì—ì„œ ê°€ì ¸ì˜¨ ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. "
                             f"í…Œì´ë¸”ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì„¸ìš” (í˜•ì œì˜ ë‹¤ë¥¸ ê·œê²©ì„ í•¨ë¶€ë¡œ í˜¼ìš©í•˜ì§€ ë§ ê²ƒ).")

    # í…Œì´ë¸” ë°ì´í„° â†’ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    tables = chunk.get("tables", [])
    for i, table in enumerate(tables):
        headers = table.get("headers", [])
        rows = table.get("rows", [])
        if not headers:
            continue

        parts.append(f"\n## í…Œì´ë¸” {i+1} (ìœ í˜•: {table.get('type', 'unknown')})")

        # Markdown í…Œì´ë¸” ìƒì„±
        parts.append("| " + " | ".join(headers) + " |")
        parts.append("| " + " | ".join(["---"] * len(headers)) + " |")
        for row in rows:
            cells = [str(row.get(h, "")) for h in headers]
            parts.append("| " + " | ".join(cells) + " |")

        # í…Œì´ë¸” ë‚´ ì£¼ì„
        notes = table.get("notes_in_table", [])
        if notes:
            parts.append(f"\ní…Œì´ë¸” ì£¼ì„: {'; '.join(str(n) for n in notes)}")

    # ì²­í¬ ì£¼ì„
    chunk_notes = chunk.get("notes", [])
    if chunk_notes:
        parts.append(f"\n## ì£¼ì„\n" + "\n".join(str(n) for n in chunk_notes))

    # cross_references
    xrefs = chunk.get("cross_references", [])
    if xrefs:
        parts.append(f"\n## êµì°¨ì°¸ì¡°")
        for xref in xrefs:
            parts.append(f"- â†’ {xref.get('target_section_id', '')} ({xref.get('context', '')[:50]})")

    parts.append(f"\n## ì§€ì‹œì‚¬í•­")
    parts.append("ìœ„ í’ˆì…ˆ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸”ì—ì„œ ì—”í‹°í‹°(ê³µì¢…, ë…¸ë¬´, ì¥ë¹„, ìì¬, ì£¼ì„, ê¸°ì¤€)ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.")

    return "\n".join(parts)


# â”€â”€â”€ LLM í˜¸ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Why: API í˜¸ì¶œì´ hangë  ê²½ìš° ë¬´í•œ ëŒ€ê¸° ë°©ì§€. 120ì´ˆ ì´ˆê³¼ ì‹œ íƒ€ì„ì•„ì›ƒ ì—ëŸ¬ ë°œìƒ.
API_TIMEOUT_SECONDS = 120


async def extract_single_chunk(
    chunk: dict,
    semaphore: asyncio.Semaphore,
    all_chunks: list[dict] = [],
) -> ChunkExtraction:
    """ë‹¨ì¼ ì²­í¬ì— ëŒ€í•´ LLM ì¶”ì¶œ ì‹¤í–‰ (ë¹„ë™ê¸°, íƒ€ì„ì•„ì›ƒ+ì¬ì‹œë„ í¬í•¨)"""
    chunk_id = chunk["chunk_id"]
    section_id = chunk["section_id"]

    async with semaphore:
        user_prompt = build_user_prompt(chunk, all_chunks)

        for attempt in range(LLM_RETRY_COUNT):
            try:
                # Why: asyncio.wait_forë¡œ 120ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •.
                #      ì´ì „ ë²„ì „ì—ì„œ íƒ€ì„ì•„ì›ƒ ì—†ì´ ë¬´í•œ hang ë°œìƒ (5ì‹œê°„+)
                api_call = asyncio.to_thread(
                    client.chat.completions.create,
                    model=LLM_MODEL,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": FEW_SHOT_EXAMPLE + "\n\n---\n\n" + user_prompt},
                    ],
                    response_format={"type": "json_object"},
                    temperature=LLM_TEMPERATURE,
                    max_tokens=LLM_MAX_TOKENS,  # ğŸ’¡ [Track A] ë§¤íŠ¸ë¦­ìŠ¤ ì „ê°œ ì‹œ ì¶œë ¥ í† í° ë¶€ì¡±(Truncation) ë°©ì§€, 16384 í™•ì¥
                )
                response = await asyncio.wait_for(
                    api_call, timeout=API_TIMEOUT_SECONDS
                )

                # íŒŒì‹±
                raw_text = response.choices[0].message.content
                llm_result = LLMExtractionResult.model_validate_json(raw_text)

                # LLM ê²°ê³¼ â†’ Phase 2 ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
                entities = []
                relationships = []

                base_conf = llm_result.confidence
                if chunk_id in ISOLATED_CHUNKS:
                    base_conf = min(0.7, base_conf)

                for le in llm_result.entities:
                    try:
                        etype = EntityType(le.type)
                    except ValueError:
                        continue  # ì˜ëª»ëœ íƒ€ì… ìŠ¤í‚µ

                    entity = Entity(
                        type=etype,
                        name=le.name,
                        spec=le.spec,
                        unit=le.unit,
                        quantity=le.quantity,
                        source_chunk_id=chunk_id,
                        source_section_id=section_id,
                        source_method="llm",
                        confidence=base_conf,
                    )
                    entities.append(entity)

                for lr in llm_result.relationships:
                    try:
                        rtype = RelationType(lr.relation_type)
                    except ValueError:
                        continue

                    source_type = _find_entity_type(lr.source, entities)
                    target_type = _find_entity_type(lr.target, entities)

                    rel = Relationship(
                        source=lr.source,
                        source_type=source_type,
                        target=lr.target,
                        target_type=target_type,
                        type=rtype,
                        quantity=lr.quantity,
                        unit=lr.unit,
                        per_unit=lr.per_unit,
                        properties=lr.properties if lr.properties else {},  # ğŸ’¡ [Track A] source_spec ì „ë‹¬
                        source_chunk_id=chunk_id,
                    )
                    relationships.append(rel)

                return ChunkExtraction(
                    chunk_id=chunk_id,
                    section_id=section_id,
                    department=chunk.get("department", ""),
                    chapter=chunk.get("chapter", ""),
                    title=chunk.get("title", ""),
                    entities=entities,
                    relationships=relationships,
                    summary=llm_result.summary,
                    confidence=base_conf,
                    source_method="llm",
                )

            except asyncio.TimeoutError:
                err_msg = f"API íƒ€ì„ì•„ì›ƒ ({API_TIMEOUT_SECONDS}ì´ˆ, ì‹œë„ {attempt+1}/{LLM_RETRY_COUNT})"
                if attempt < LLM_RETRY_COUNT - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return ChunkExtraction(
                        chunk_id=chunk_id, section_id=section_id,
                        department=chunk.get("department", ""),
                        chapter=chunk.get("chapter", ""),
                        title=chunk.get("title", ""),
                        source_method="llm", confidence=0.0,
                        warnings=[err_msg],
                    )
            except Exception as e:
                if attempt < LLM_RETRY_COUNT - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return ChunkExtraction(
                        chunk_id=chunk_id, section_id=section_id,
                        department=chunk.get("department", ""),
                        chapter=chunk.get("chapter", ""),
                        title=chunk.get("title", ""),
                        source_method="llm", confidence=0.0,
                        warnings=[f"LLM ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {LLM_RETRY_COUNT}íšŒ): {str(e)[:200]}"],
                    )


def _find_entity_type(name: str, entities: list[Entity]) -> EntityType:
    """ì—”í‹°í‹° ëª©ë¡ì—ì„œ ì´ë¦„ìœ¼ë¡œ íƒ€ì… ì°¾ê¸°"""
    for e in entities:
        if e.name == name:
            return e.type
    return EntityType.WORK_TYPE  # ê¸°ë³¸ê°’


# â”€â”€â”€ ëŒ€ìƒ ì²­í¬ í•„í„°ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def select_llm_target_chunks(
    chunks: list[dict],
    step1_result: BatchResult | None,
) -> list[dict]:
    """Step 2.1 ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ LLM ì¶”ì¶œì´ í•„ìš”í•œ ì²­í¬ë¥¼ ì„ ë³„

    ëŒ€ìƒ:
    1. í…Œì´ë¸”ì´ ì•„ì˜ˆ ì—†ëŠ” ì²­í¬ (í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ)
    2. D_ê¸°íƒ€/C_êµ¬ë¶„ì„¤ëª…ë§Œ ìˆëŠ” ì²­í¬ (ê·œì¹™ ì¶”ì¶œ ë¯¸ëŒ€ìƒ)
    3. Step 2.1ì—ì„œ WorkTypeì´ 0ê°œì¸ ì²­í¬ (ë³´ê°• í•„ìš”)
    4. Step 2.1ì—ì„œ ê²½ê³ ê°€ ìˆëŠ” ì²­í¬ (ì¸ì‹ ë¶ˆê°€ í—¤ë”)
    """
    # Step 2.1 ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì „ì²´ ëŒ€ìƒ
    if step1_result is None:
        return chunks

    # Step 2.1 ê²°ê³¼ë¥¼ chunk_idë³„ë¡œ ì¸ë±ì‹±
    step1_map = {e.chunk_id: e for e in step1_result.extractions}

    targets = []
    reasons = Counter()

    for chunk in chunks:
        chunk_id = chunk["chunk_id"]
        s1 = step1_map.get(chunk_id)

        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ (ì„¹ì…˜ ì œëª©ë§Œ ìˆëŠ” ê²½ìš°)
        text = chunk.get("text", "")
        tables = chunk.get("tables", [])
        if len(text) < 20 and not tables:
            reasons["í…ìŠ¤íŠ¸ ë„ˆë¬´ ì§§ìŒ (ìŠ¤í‚µ)"] += 1
            continue

        if s1 is None:
            targets.append(chunk)
            reasons["Step 2.1 ê²°ê³¼ ì—†ìŒ"] += 1
            continue

        # ì¡°ê±´ 1: í…Œì´ë¸” ì—†ëŠ” í…ìŠ¤íŠ¸ ì²­í¬
        if not tables and len(text) > 30:
            targets.append(chunk)
            reasons["í…Œì´ë¸” ì—†ìŒ (í…ìŠ¤íŠ¸ ì¶”ì¶œ)"] += 1
            continue

        # ì¡°ê±´ 2: D_ê¸°íƒ€/C_êµ¬ë¶„ì„¤ëª…ë§Œ ìˆëŠ” í…Œì´ë¸”
        # ë‹¨, step1ì—ì„œ ì´ë¯¸ WorkTypeì„ ì¶”ì¶œí•œ ì²­í¬ëŠ” ì œì™¸ (ë§¤íŠ¸ë¦­ìŠ¤ ì¶”ì¶œ ì„±ê³µ)
        table_types = {t.get("type", "") for t in tables}
        if table_types <= {"D_ê¸°íƒ€", "C_êµ¬ë¶„ì„¤ëª…"}:
            has_worktype = any(e.type == EntityType.WORK_TYPE for e in s1.entities)
            if not has_worktype:
                targets.append(chunk)
                reasons["D_ê¸°íƒ€/C_êµ¬ë¶„ì„¤ëª… í…Œì´ë¸”ë§Œ (WorkType ì—†ìŒ)"] += 1
                continue

        # ì¡°ê±´ 3: WorkType ì¶”ì¶œ ì•ˆ ë¨
        has_worktype = any(e.type == EntityType.WORK_TYPE for e in s1.entities)
        if not has_worktype and s1.entities:
            targets.append(chunk)
            reasons["WorkType ë¯¸ì¶”ì¶œ"] += 1
            continue

        # ì¡°ê±´ 4: ê²½ê³  ì¡´ì¬ (ì¸ì‹ ë¶ˆê°€ í—¤ë” ë“±)
        meaningful_warnings = [w for w in s1.warnings if "í…Œì´ë¸” ì—†ìŒ" not in w]
        if meaningful_warnings:
            targets.append(chunk)
            reasons["Step 2.1 ê²½ê³  ìˆìŒ"] += 1
            continue

    return targets, reasons


# â”€â”€â”€ ë©”ì¸ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€â”€ ì¤‘ê°„ ì €ì¥ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PARTIAL_SAVE_FILE = PHASE2_OUTPUT / "llm_entities_partial.json"
SAVE_INTERVAL = 200  # 200ê±´ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥


def save_partial_result(result: BatchResult):
    """ì¤‘ê°„ ê²°ê³¼ ì €ì¥. crash ì‹œì—ë„ ì§„í–‰ ìƒíƒœ ë³´ì¡´."""
    PHASE2_OUTPUT.mkdir(parents=True, exist_ok=True)
    PARTIAL_SAVE_FILE.write_text(
        result.model_dump_json(indent=2),
        encoding="utf-8",
    )


def load_existing_chunk_ids() -> set[str]:
    """ì´ì–´í•˜ê¸°(resume): ê¸°ì¡´ ê²°ê³¼ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ chunk_id ë¡œë“œ"""
    done_ids = set()
    for path in [LLM_ENTITIES_FILE, PARTIAL_SAVE_FILE]:
        if path.exists():
            try:
                data = json.loads(path.read_text(encoding="utf-8"))
                for ext in data.get("extractions", []):
                    done_ids.add(ext["chunk_id"])
            except Exception:
                pass
    return done_ids


def load_existing_extractions() -> list[ChunkExtraction]:
    """ì´ì–´í•˜ê¸°: ê¸°ì¡´ ê²°ê³¼ì—ì„œ extractions ë¡œë“œ"""
    existing = []
    for path in [LLM_ENTITIES_FILE, PARTIAL_SAVE_FILE]:
        if path.exists():
            try:
                data = json.loads(path.read_text(encoding="utf-8"))
                for ext_data in data.get("extractions", []):
                    existing.append(ChunkExtraction.model_validate(ext_data))
            except Exception:
                pass
    # chunk_id ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ë‚˜ì¤‘ ê²ƒ ìš°ì„ )
    seen = {}
    for ext in existing:
        seen[ext.chunk_id] = ext
    return list(seen.values())


async def run_step2_async(sample: bool = False, resume: bool = False, section_filter: str = None) -> BatchResult:
    """Step 2.2 ë¹„ë™ê¸° ì‹¤í–‰

    Args:
        sample: Trueë©´ 20ê°œë§Œ ì²˜ë¦¬
        resume: Trueë©´ ê¸°ì¡´ ê²°ê³¼ì—ì„œ ì´ì–´ì„œ ì²˜ë¦¬
        section_filter: ì§€ì •ëœ ë¬¸ìì—´ì´ section_id ì— í¬í•¨ëœ ì²­í¬ë§Œ ì¶”ì¶œ
    """
    print("\n  Step 2.2: LLM ê¸°ë°˜ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ (DeepSeek-V3)")
    print("  " + "=" * 55)
    print(f"  íƒ€ì„ì•„ì›ƒ: {API_TIMEOUT_SECONDS}ì´ˆ/ìš”ì²­, ë™ì‹œì„±: {LLM_CONCURRENCY}")

    # ë°ì´í„° ë¡œë“œ
    data = json.loads(CHUNKS_FILE.read_text(encoding="utf-8"))
    chunks = data["chunks"]

    # Step 2.1 ê²°ê³¼ ë¡œë“œ
    step1_result = None
    if TABLE_ENTITIES_FILE.exists():
        step1_data = json.loads(TABLE_ENTITIES_FILE.read_text(encoding="utf-8"))
        step1_result = BatchResult.model_validate(step1_data)
        print(f"  Step 2.1 ê²°ê³¼ ë¡œë“œ: {step1_result.total_entities} ì—”í‹°í‹°")

    # ëŒ€ìƒ ì²­í¬ ì„ ë³„
    if section_filter:
        targets = [c for c in chunks if section_filter in c.get("section_id", "") or section_filter in c.get("subsection", "") or section_filter in c.get("title", "")]
        reasons = Counter({"ì„¹ì…˜ í•„í„° ê°•ì œ ì§€ì •": len(targets)})
        print(f"  [ì„¹ì…˜ í•„í„°] '{section_filter}' ì ìš©: ê°•ì œ ì¶”ì¶œ ëŒ€ìƒ {len(targets)}ê°œ ì²­í¬ ë°œê²¬")
    else:
        targets, reasons = select_llm_target_chunks(chunks, step1_result)

    # ì´ì–´í•˜ê¸°: ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ
    existing_extractions = []
    if resume and not section_filter:  # ì„¹ì…˜ ì§€ì • í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì´ì–´í•˜ê¸° ìŠ¤í‚µ
        done_ids = load_existing_chunk_ids()
        existing_extractions = load_existing_extractions()
        before = len(targets)
        targets = [c for c in targets if c["chunk_id"] not in done_ids]
        print(f"  [ì´ì–´í•˜ê¸°] ê¸°ì¡´ {len(done_ids)}ê±´ ìŠ¤í‚µ, ì”ì—¬ {len(targets)}/{before}ê±´")

    if sample and not section_filter:
        targets = targets[:20]
        print(f"  [ìƒ˜í”Œ ëª¨ë“œ] {len(targets)}ê°œ ì²­í¬ë§Œ ì²˜ë¦¬")

    print(f"\n  LLM ì¶”ì¶œ ëŒ€ìƒ: {len(targets)}ê°œ ì²­í¬")
    for reason, cnt in reasons.most_common():
        print(f"    {reason}: {cnt}")

    if not targets:
        print("  ì¶”ì¶œ ëŒ€ìƒ ì—†ìŒ. ì¢…ë£Œ.")
        if existing_extractions:
            # ê¸°ì¡´ ê²°ê³¼ë§Œìœ¼ë¡œ ìµœì¢… íŒŒì¼ ìƒì„±
            result = BatchResult(total_chunks=len(existing_extractions))
            result.extractions = existing_extractions
            result.processed_chunks = len(existing_extractions)
            _finalize_result(result)
            return result
        return BatchResult()

    # ë¹„ë™ê¸° ì¶”ì¶œ ì‹¤í–‰
    semaphore = asyncio.Semaphore(LLM_CONCURRENCY)
    result = BatchResult(total_chunks=len(targets) + len(existing_extractions))
    result.extractions = list(existing_extractions)  # ê¸°ì¡´ ê²°ê³¼ í¬í•¨
    result.processed_chunks = len(existing_extractions)

    print(f"\n  ì²˜ë¦¬ ì‹œì‘ (ë™ì‹œ {LLM_CONCURRENCY}ê°œ, íƒ€ì„ì•„ì›ƒ {API_TIMEOUT_SECONDS}ì´ˆ)...")
    sys.stdout.flush()
    start_time = time.time()

    tasks = [extract_single_chunk(c, semaphore, chunks) for c in targets]

    # ì§„í–‰ë¥  í‘œì‹œ + ì¤‘ê°„ ì €ì¥
    completed = 0
    for coro in asyncio.as_completed(tasks):
        extraction = await coro
        result.extractions.append(extraction)
        result.processed_chunks += 1
        completed += 1

        if completed % 50 == 0 or completed == len(tasks):
            elapsed = time.time() - start_time
            rate = completed / elapsed if elapsed > 0 else 0
            remaining = (len(tasks) - completed) / rate if rate > 0 else 0
            print(f"    [{completed:4d}/{len(tasks)}] "
                  f"{elapsed:.0f}ì´ˆ ({rate:.1f}ê±´/ì´ˆ) "
                  f"ì”ì—¬ ~{remaining:.0f}ì´ˆ")
            sys.stdout.flush()

        # Why: 200ê±´ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥ â†’ í”„ë¡œì„¸ìŠ¤ crash ì‹œì—ë„ ê²°ê³¼ ë³´ì¡´
        if completed % SAVE_INTERVAL == 0:
            save_partial_result(result)

    elapsed = time.time() - start_time

    # ìµœì¢… í†µê³„ & ì €ì¥
    _finalize_result(result)

    print(f"\n  ì™„ë£Œ ({elapsed:.0f}ì´ˆ ì†Œìš”, {elapsed/60:.1f}ë¶„)")
    return result


def _finalize_result(result: BatchResult):
    """í†µê³„ ì§‘ê³„ + ìµœì¢… íŒŒì¼ ì €ì¥"""
    entity_type_counter = Counter()
    rel_type_counter = Counter()
    failed_count = 0

    for ext in result.extractions:
        if ext.confidence == 0.0:
            failed_count += 1
            result.failed.append({
                "chunk_id": ext.chunk_id,
                "warnings": ext.warnings,
            })
        for e in ext.entities:
            entity_type_counter[e.type.value] += 1
        for r in ext.relationships:
            rel_type_counter[r.type.value] += 1

    result.total_entities = sum(entity_type_counter.values())
    result.total_relationships = sum(rel_type_counter.values())
    result.entity_type_counts = dict(entity_type_counter)
    result.relationship_type_counts = dict(rel_type_counter)

    # ìµœì¢… ì €ì¥
    PHASE2_OUTPUT.mkdir(parents=True, exist_ok=True)
    LLM_ENTITIES_FILE.write_text(
        result.model_dump_json(indent=2),
        encoding="utf-8",
    )

    # ë¦¬í¬íŠ¸ ì¶œë ¥
    print(f"\n  ê²°ê³¼:")
    print(f"    ì²˜ë¦¬ ì²­í¬: {result.processed_chunks}/{result.total_chunks}")
    print(f"    ì„±ê³µ: {result.processed_chunks - failed_count} / ì‹¤íŒ¨: {failed_count}")
    print(f"    ì´ ì—”í‹°í‹°: {result.total_entities}")
    for etype, cnt in sorted(entity_type_counter.items(), key=lambda x: -x[1]):
        print(f"      {etype}: {cnt}")
    print(f"    ì´ ê´€ê³„: {result.total_relationships}")
    for rtype, cnt in sorted(rel_type_counter.items(), key=lambda x: -x[1]):
        print(f"      {rtype}: {cnt}")

    print(f"\n  ì €ì¥: {LLM_ENTITIES_FILE}")

    # partial íŒŒì¼ ì •ë¦¬
    if PARTIAL_SAVE_FILE.exists():
        PARTIAL_SAVE_FILE.unlink()


def run_step2(sample: bool = False, resume: bool = False, section_filter: str = None) -> BatchResult:
    """ë™ê¸° ë˜í¼"""
    return asyncio.run(run_step2_async(sample, resume, section_filter))


if __name__ == "__main__":
    sample_mode = "--sample" in sys.argv
    resume_mode = "--resume" in sys.argv
    
    section_arg = None
    if "--section" in sys.argv:
        idx = sys.argv.index("--section")
        section_arg = sys.argv[idx + 1]
        
    run_step2(sample=sample_mode, resume=resume_mode, section_filter=section_arg)
